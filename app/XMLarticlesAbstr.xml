<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<articles xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
	<article>
		<articleId>698</articleId>
		<title>Query Processing in a Self-Organized Storage System</title>
		<trackName>PhD Workshop</trackName>
		<abstract>The amount of scalability and robustness provided by current solutions for the storage and retrieval of data might not be sufficient to support ever-larger web applications. By using swarm intelligence to route operations in a distributed storage system, these limitations can be overcome. However, the possibilities for the efficient evaluation of complex queries in this kind of system are scarce and have not been researched yet. Based on a schema-less data model along with the building blocks for complex queries on this model, I present an approach for complex query processing as part of my PhD work. Here, complex queries are moved through the distributed storage system, while constantly being re-optimized using strictly local information. The approach is described along with an evaluation methodology and a test protocol. My goal is to contribute complex query processing for a fully distributed storage system with unreliable basic read operations and probabilistic directional content routing.</abstract>
	</article>
	<article>
		<articleId>796</articleId>
		<title>Matching tree patterns on partial-trees</title>
		<trackName>PhD Workshop</trackName>
		<abstract>Tree patterns are query language that are used to simplify processing while making it more efficient. The existing methodologies for tree pattern matching in database (DB) process strings of labels that annotate nodes in the tree. The existing structural-join and structural-index techniques process strings of labels. My thesis focuses on developing methodologies that enable structural-join and structural-index processing on labels of nodes in subtrees.   This thesis introduces a simple tree model that is called partial-tree that models both tree patterns and tree data as partialtrees. Partial-trees supply an infrastructure for efficient processing of both structural-join and structural-index on subtrees. This thesis develops two methods for pattern matching: multi-scale index and sub-linear join. Tree pattern matching in partial-trees also enables to perform query on data that is partially known such as probabilistic or experimental data.   The thesis suggests an automata based approach that models partial-trees as tree automata. It defines a methodology that matches partial-tree patterns, which are formalized as tree automata, in a tree. It also defines how to match partial-tree patterns, which are formalized as tree automata, in partial-tree data that is also formalized as tree automata.</abstract>
	</article>
	<article>
		<articleId>803</articleId>
		<title>Efficient Top-k Searching According to User Preferences Based on Fuzzy Functions With Usage of Tree-Oriented Data Structures</title>
		<trackName>PhD Workshop</trackName>
		<abstract>In the last few years, research of top-k processing is in progress in various domains. The aim of our research is efficient top-k searching of the best k objects with more attributes according to more complex user preferences. We focus on a multi-user solution, where data is common for all users. We use a model of user preferences based on fuzzy functions, where each user can express his/her preferences for each attribute by a fuzzy function and mutual relations between the attributes by an aggregation function. In our previous research we focused on top-k searching in tree-oriented data structures and we developed various top-k algorithms, which solve top-k problem efficiently and support the model of user preferences. Furthermore, we assume that attributes of an object type can be distributed in various information resources, e.g. data streams, web services, remote RDBMS, and so on. Therefore, integration of data is one of the topics of our research. Our research also focuses on development of the TREETOPK software system that will be including our new-developed technologies and experiments with real data from various information resources. </abstract>
	</article>
	<article>
		<articleId>812</articleId>
		<title>Mixed Workload Management for In-Memory Databases</title>
		<trackName>PhD Workshop</trackName>
		<abstract>The fast-paced business environment in most industries requires analytical applications to report on the latest data of an enterprise. Today, organizations either rely on data warehouses, which make flexible reporting on up-to-date data almost impossible, or maintain copies of operational data, leading to increased costs and architectural complexity. In-memory database management systems (DBMS) on multicore CPUs are a promising platform to run analytical queries (OLAP) directly on operational data, in addition to processing load from transactional queries (OLTP). To meet response time requirements of both types of queries on a single system, some form of workload management has to be implemented to avoid system overload. Consequently, the early-stage PhD project described in this paper targets to design a workload manager optimized for mixed workloads on in-memory databases. At this stage, we envision a DBMS-internal workload management system that assigns as many resources to OLTP queries as necessary to meet response time requirements and schedules OLAP queries on the remaining resources. To minimize overall OLAP response time, we intend to leverage the underlying multicore architecture by scheduling available resources on the level of atomic database tasks, thereby considering cache behavior and the degree of parallelism. We believe that existing solutions in this area fall short for two reasons: (1) they do not consider the different response time requirements of both, operational and analytical applications, and (2) they do not take characteristics of modern in-memory DBMS, such as cache access patterns and massive parallel execution, into account. Our findings will be verified in the context of enterprise computing by using workloads from real-life enterprise systems.</abstract>
	</article>
	<article>
		<articleId>813</articleId>
		<title>Knowledge-Based Complex Event Processing</title>
		<trackName>PhD Workshop</trackName>
		<abstract>Event-driven systems are highly depending on the quality of detection and processing of events. Many of complex real-world events cannot be processed by the existing event processing systems because they are too complex to be understood and processed by the systems. Complex events can be inferred from raw primitive events based on their incoming sequence, their syntax and semantics. Usage of ontological knowledge about events and their relationship to other non-event concepts in the application domain, can improve the quality of event processing. In this Ph.D. thesis, I am aiming to address the challenges of adding formalized vocabularies/ontologies and declarative rules to the area of event processing for enabling more intelligent event processors which can understand the senses and semantics of events.</abstract>
	</article>
	<article>
		<articleId>825</articleId>
		<title>Top-k Web Service Compositions in the context of user preferences</title>
		<trackName>PhD Workshop</trackName>
		<abstract>Data-driven Web service composition is a powerful means to answer users' complex  queries. User preferences are a key aspect that must be taken into account in the composition  scheme. In this paper, we present an approach to automatically compose Data-drive Web services while taking into account  the user preferences. User preferences are modeled using fuzzy sets, then incorporated into the composition queries. We use an RDF query rewriting algorithm to determine the relevant services. The fuzzy  constraints of the relevant services are matched to those of the query using a set of matching methods. We define a criterion to rank-order available services using a fuzzification of Pareto dominance, then compute the top-k service compositions. We propose also a method to improve the diversity of returned compositions while maintaining as possible the  compositions with the highest ranking ones. Finally, we present the architecture of our system.</abstract>
	</article>
	<article>
		<articleId>832</articleId>
		<title>Scaling Web Applications: A Temporal Approach</title>
		<trackName>PhD Workshop</trackName>
		<abstract>In this paper we present the theoretical aspects of a consistent and scalable web application framework. In this framework the server handles all updates and maintains a centralized temporal database, but only answers queries used for replicating snapshot databases for pre-defined periods. These database snapshots are subsequently replicated to the client-side database to answer the general web queries. The scalability is improved because most queries don’t require synchronization, and in case synchronization is required, it makes effective use of the existing web cache to reduce latency and queries on the centralized database. We present the theoretic underpinning of this approach as well as the tradeoffs imposed.</abstract>
	</article>
	<article>
		<articleId>833</articleId>
		<title>Research on a Schema and Data Versioning System</title>
		<trackName>PhD Workshop</trackName>
		<abstract>In this paper, I describe my dissertation research.  This primarily involves creating a framework for effectively managing the inevitable evolution of schema in an Online Transaction Processing (OLTP) database.  This framework is based on common practices used to manage source code changes in software development.  It allows users of a database to create sandboxes in which changes to the database are isolated from the main database and from other sandboxes. Schema versioning techniques are used to isolate schema changes made within sandboxes and to allow queries executed in a sandbox to retrieve data from the main database without converting the database to the schema associated with the sandbox.  The framework also includes a mechanism for handling data versioning within each sandbox.  The combination of these two versioning techniques provides a system that significantly reduces the effort, storage capacity, and infrastructure required to perform development, maintenance, and testing of schema and data changes against a high-volume online database.</abstract>
	</article>
	<article>
		<articleId>372</articleId>
		<title>A Framework for Supporting DBMS-like Indexes in the Cloud</title>
		<trackName>Research</trackName>
		<abstract>To support ``Database as a service'' (DaaS) in the cloud, the database system is expected to provide similar functionalities as in centralized DBMS such as efficient processing of ad hoc queries. The system must therefore support DBMS-like indexes, possibly a few indexes for each table to provide fast location of data distributed over the network.  In such a distributed environment, the indexes have to be distributed over the network to achieve scalability and reliability. Each cluster node maintains a subset of the index data. As in conventional DBMS, indexes incur maintenance overhead and the problem is more complex in the distributed environment since the data are typically partitioned and distributed based on a subset of attributes. Further, the distribution of indexes is not straight forward, and there is therefore always a question on scalability, in terms of data volume, network size, and number of indexes.  In this paper, we examine the problem of providing DBMS-like indexing mechanisms in the cloud database systems, and propose an extensible, but simple and efficient indexing framework that enables users to define their own type of indexes without knowing the structure of the underlying network. It is also designed to ensure the efficiency of hopping between cluster nodes during index traversal, and reduce the maintenance cost of indexes. We implement three common indexes, namely hash indexes, distributed B$^+$-tree-like indexes and distributed multi-dimensional indexes, to demonstrate the usability and effectiveness of the framework. We conduct experiments on Amazon EC2 and an in-house cluster to verify the efficiency and scalability of the framework.            </abstract>
	</article>
	<article>
		<articleId>376</articleId>
		<title>On Link-based Similarity Join</title>
		<trackName>Research</trackName>
		<abstract>Graphs can be found in applications like social networks, bibliographic networks, and biological databases. Understanding the relationship, or links, among graph nodes enables applications such as link prediction,  recommendation, and spam detection. In this paper, we propose link-based similarity join (LS-join), which extends the similarity join operator to link-based measures. Given two sets of nodes in a graph, the LS-join returns all pairs of nodes that are highly similar to each other, with respect to an e-function. The e-function generalizes common measures like Personalized PageRank (PPR) and SimRank (SR). We study an efficient LS-join algorithm on a large graph. We further improve our solutions for PPR and SR, which involve expensive random-walk operations. We validate our solutions by performing extensive experiments on three real graph datasets.</abstract>
	</article>
	<article>
		<articleId>377</articleId>
		<title>On Querying Historical Evolving Graph Sequences</title>
		<trackName>Research</trackName>
		<abstract>In many applications, information is best represented as graphs.  In a dynamic world, information changes and so the graphs representing the information evolve with time.  We propose that historical graph-structured data be maintained for analytical processing.  We call a historical evolving graph sequence an EGS.  We observe that in many applications, graphs of an EGS are large and numerous, and they often exhibit much redundancy among them.  We study the problem of efficient query processing on an EGS and put forward a solution framework called FVF.  Through extensive experiments on both real and synthetic datasets, we show that our FVF framework is highly efficient in EGS query processing.</abstract>
	</article>
	<article>
		<articleId>386</articleId>
		<title>RemusDB: Transparent High-Availability for Database Systems</title>
		<trackName>Research</trackName>
		<abstract>In this paper we present a technique for building a high-availability (HA) database management system (DBMS). The proposed technique can be applied to any DBMS with little or no customization, and with reasonable performance overhead. Our approach is based on Remus, a commodity HA solution implemented in the virtualization layer, that uses asynchronous virtual machine (VM) state replication to provide transparent HA and failover capabilities. We show that while Remus and similar systems can protect a DBMS, database workloads incur a performance overhead of up to 32% as compared to an unprotected DBMS. We identify the sources of this overhead and develop optimizations that mitigate the problems. We present an experimental evaluation using two popular database systems and industry standard benchmarks showing that for certain workloads, our optimized approach provides very fast failover (3 seconds of downtime) with low performance overhead when compared to an unprotected DBMS. Our approach provides a practical  means for existing, deployed database systems to be made more reliable with a minimum of risk, cost, and effort. Furthermore, this paper invites new discussion about whether the complexity of HA is best implemented within the DBMS, or as a service by the infrastructure below it. </abstract>
	</article>
	<article>
		<articleId>388</articleId>
		<title>Completeness of Queries over Incomplete Databases</title>
		<trackName>Research</trackName>
		<abstract>Data completeness is an important aspect of data quality as in many scenarios it is crucial to guarantee completeness of query answers. We develop techniques to conclude the completeness of query answers from information about the completeness of parts of a generally incomplete database.  In our framework, completeness of a database can be described in two ways: by table completeness (TC) statements, which say that certain parts of a relation are complete, and by query completeness (QC) statements, which say that the set of answers of a query is complete. We identify as core problem to decide whether table completeness entails query completeness (TC-QC). We develop decision procedures and assess the complexity of TC-QC inferences depending on the languages of the TC and QC statements. We show that in important cases weakest preconditions for query completeness can be expressed in terms of table completeness statements, which means that these statements identify precisely the parts of a database that are critical for the completeness of a query. For the related problem of QC-QC entailment, we discuss its connection to query determinacy. Moreover, we show how to use the concrete state of a database to enable further completeness inferences.</abstract>
	</article>
	<article>
		<articleId>401</articleId>
		<title>A Subsequence Matching with Gaps-Range-Tolerances Framework: A Query-By-Humming Application</title>
		<trackName>Research</trackName>
		<abstract>We propose a novel subsequence matching framework that allows for gaps in both the query and target sequences, variable matching tolerance levels efficiently tuned for each query and target sequence, and also constrains the maximum match length. Using this framework, a space and time efficient dynamic programming method is developed: given a short query sequence and a large database, our method identifies the subsequence of the database that best matches the query, and further bounds the number of consecutive gaps in both sequences. In addition, it allows the user to constrain the minimum number of matching elements between a query and a database sequence. We show that the proposed method is highly applicable to music retrieval. Music pieces are represented by 2-dimensional time series, where each dimension holds information about the pitch and duration of each note, respectively. At runtime, the query song is transformed to the same 2-dimensional representation. We present an extensive experimental evaluation using synthetic and hummed queries on a large music database. Our method outperforms, in terms of accuracy, several DP-based subsequence matching methods---with the same time complexity---and a probabilistic model-based method.</abstract>
	</article>
	<article>
		<articleId>412</articleId>
		<title>Approximate Substring Matching over Uncertain Strings</title>
		<trackName>Research</trackName>
		<abstract>Text data is prevalent in life. Some of this data is uncertain and is best modeled by probability distributions. Examples include bio-logical sequence data and automatic ECG annotations, among others. Approximate substring matching over uncertain texts is largely an unexplored problem in data management. In this paper, we study this intriguing question. We propose a semantics called (k, τ)-matching queries and argue that it is more suitable in this context than a related semantics that has been proposed previously. Since uncertainty incurs considerable overhead on indexing as well as the final verification for a match, we devise techniques for both. For indexing, we propose a multilevel filtering technique based on measuring signature distance; for verification, we design two algorithms that give upper and lower bounds and significantly reduce the costs. We validate our algorithms with a systematic evaluation on two real-world datasets and some synthetic datasets.</abstract>
	</article>
	<article>
		<articleId>421</articleId>
		<title>Serializable Snapshot Isolation for Replicated Databases in High-Update Scenarios</title>
		<trackName>Research</trackName>
		<abstract>Many proposals for managing replicated data use sites running the Snapshot Isolation (SI) concurrency control mechanism, and provide 1-copy SI or something similar, as the global isolation level. This allows good scalability, since only ww-conflicts need to be managed globally. However, 1-copy SI can lead to data corruption and violation of integrity constraints. 1-copy serializability is the global correctness condition that prevents data corruption. We propose a new algorithm Replicated Serializable Snapshot Isolation (RSSI) that uses SI at each site, and combines this with a certification algorithm to guarantee 1-copy serializable global execution. Management of ww-conflicts is similar to what is done in 1-copy SI. But unlike previous designs for 1-copy serializable systems, we do not need to prevent all rw-conflicts among concurrent transactions. We formalize this in a theorem that shows that many rw-conflicts are indeed false positives that do not risk non-serializable behavior. Our proposed RSSI algorithm will only abort a transaction when it detects a well-defined pattern of two consecutive  rw-edges in the serialization graph. We have built a prototype that integrates our RSSI with the existing open-source Postgres-R(SI) system. Our performance evaluation shows that there is a worst-case overhead of about 15% for getting full 1-copy serializability as compared to 1-copy SI in a cluster of 8 nodes, with our proposed RSSI clearly outperforming the previous work for update-intensive workloads.</abstract>
	</article>
	<article>
		<articleId>422</articleId>
		<title>PALM: Parallel Architecture-Friendly Latch-Free Modifications to B+ Trees on Many-Core Processors</title>
		<trackName>Research</trackName>
		<abstract>Concurrency control on B+ trees is primarily achieved with latches, but serialization and contention can hinder scalability. As core counts on current processors increase, it is imperative to develop scalable latch-free techniques for concurrency control. We present PALM, a novel technique for performing multiple concurrent queries on in-memory B+ trees. PALM is based on the Bulk Synchronous Parallel model, which guarantees freedom from deadlocks and race conditions. Input queries are grouped and processed in atomic batches, and work proceeds in stages that preclude contention. Transition between stages is accomplished with scalable point-to-point communication. PALM exploits data-and thread-level parallelism on modern many-core architectures, and performs 40M updates/second on trees with 128M keys, and 128M updates/second on trees with 512K keys on the latest CPU architectures. Our throughput is 2.3X–19X that of state-of-the-art concurrent update algorithms on in-memory B+ trees. PALM obtains close to peak throughput at very low response times of less than 350microseconds, even for large trees. We also evaluate PALM on the Intel(R) Many Integrated Core (Intel(R) MIC) architecture, and demonstrate a speedup of 1.5–2.1X for out-of-cache tree sizes on an Intel(R) Knights Ferry over a pair of Intel(R) Xeon(R) processors DP X5680 (Westmere-EP)in a dual-socket configuration.</abstract>
	</article>
	<article>
		<articleId>424</articleId>
		<title>Mining Top-K Large Structural Patterns in a Massive Network</title>
		<trackName>Research</trackName>
		<abstract>With ever-growing popularity of social networks, web and bio-networks, mining large frequent patterns from a single huge network has become increasingly important. Yet the existing pattern mining methods cannot offer the efficiency desirable for large pattern discovery. We propose \textsf{SpiderMine}, a novel algorithm to efficiently mine top-$K$ largest frequent patterns from a single massive network with any user-specified probability of $1-\epsilon$. Deviating from the existing edge-by-edge (\ie, incremental) pattern-growth framework, \textsf{SpiderMine} achieves its efficiency by unleashing the power of small patterns of a bounded diameter, which we call ``spiders''.  With the spider structure, our approach adopts a probabilistic mining framework to find the top-$k$ largest patterns by (i) identifying an affordable set of promising growth paths toward large patterns, and (ii) generating large patterns with much lower combinatorial complexity, and finally (iii) greatly reducing the cost of graph isomorphism tests with a new graph pattern representation by a multi-set of spiders. Extensive experimental studies on both synthetic and real data sets show that our algorithm outperforms existing methods.</abstract>
	</article>
	<article>
		<articleId>425</articleId>
		<title>Structure-Aware Sampling: Flexible and Accurate Summarization</title>
		<trackName>Research</trackName>
		<abstract>In processing large quantities of data, a fundamental problem is to obtain a summary which supports approximate query answering.  Random sampling yields flexible summaries which naturally support subset-sum queries with unbiased estimators and well-understood confidence bounds.  Classic sample-based summaries, however, are designed for arbitrary subset queries and are oblivious to the structure in the set of keys. The particular structure, such as hierarchy, order, or product space (multi-dimensional), makes {\em range queries} much more relevant for most analysis of the data.  Dedicated summarization algorithms for range-sum queries have also been extensively studied.  They can outperform existing sampling schemes in terms of accuracy on range queries per summary size. Their accuracy, however, rapidly degrades when, as is often the case, the query spans multiple ranges.   They are also less flexible---being targeted for range sum queries alone---and are often quite costly to build and use.  In this paper we propose and evaluate variance optimal sampling schemes that are {\em structure-aware}. These summaries improve over  the accuracy of existing {\em structure-oblivious} sampling schemes on range queries while retaining the benefits of sample-based summaries: flexible summaries, with high accuracy on  both range queries and arbitrary subset queries.</abstract>
	</article>
	<article>
		<articleId>431</articleId>
		<title>Data Coordination: Supporting Contingent Updates</title>
		<trackName>Research</trackName>
		<abstract>In many scenarios, a contingent data source may benefit by coordinating with external heterogeneous sources upon which it depends.  The administrator of this contingent source needs to update it when changes are made do the external base sources.  For example, when a building design is updated, a contractor's estimate of construction costs must be updated, too.  The goal of data coordination is to update a contingent source C based on changes to an independently maintained base source B.  This paper introduces a data coordination system which allows C to coordinate its data without imposing signifi- cant requirements on B. Our system uses declarative map- pings between B and C and performs coordination in two stages View Differencing — finding changes to an interme- diate view of B based on its mapping to C, and Update Translation — translating the view differencing result into updates on C. We present and evaluate novel solutions to both stages and demonstrate their feasibility on real world problems.</abstract>
	</article>
	<article>
		<articleId>435</articleId>
		<title>Accellerating Queries with Group-By and Join by Groupjoin</title>
		<trackName>Research</trackName>
		<abstract>Most aggregation queries contain both group-by and join operators, and spend a significant amount of time evaluating these two expensive operators. Merging them into one operator (the groupjoin) significantly speeds up query execution.  We introduce two main equivalences to allow for the merging and prove their correctness. Furthermore, we show experimentally that these equivalences can significantly speed up TPC-H. </abstract>
	</article>
	<article>
		<articleId>449</articleId>
		<title>Lightweight Graphical Models for Selectivity Estimation Without Independence Assumptions</title>
		<trackName>Research</trackName>
		<abstract>As a result of decades of research and industrial development, modern query optimizers are complex software artifacts. However, the quality of the query plan chosen by an optimizer is largely determined by the quality of the underlying statistical summaries. Small selectivity estimation errors, propagated exponentially, can lead to severely sub-optimal plans. Modern optimizers typically maintain one-dimensional statistical summaries and make the attribute value independence and join uniformity assumptions for efficiently estimating selectivities. Therefore, selectivity estimation errors in today's optimizers are frequently caused by missed correlations between attributes. We present a selectivity estimation approach that does not make the independence assumptions. By carefully using concepts from the field of graphical models, we are able to factor the joint probability distribution of all the attributes in the database into small, usually two-dimensional distributions. We describe several optimizations that can make selectivity estimation highly efficient, and we present a complete implementation inside PostgreSQL's query optimizer. Experimental results indicate an order of magnitude better selectivity estimates, while keeping optimization time in the range of tens of milliseconds.</abstract>
	</article>
	<article>
		<articleId>455</articleId>
		<title>Dissemination of Models over Time-Varying Data</title>
		<trackName>Research</trackName>
		<abstract>Dissemination of time-varying data is essential in many applications, such as sensor  networks, patient monitoring, stock tickers, etc. Often, the raw data have to go  through some form of pre-processing, such as cleaning, smoothing, etc, before being  disseminated. Such pre-processing often applies mathematical or statistical models  to transform the large volumes of raw, point-based data into a much smaller number  of piece-wise continuous functions. In such cases, the necessity to distribute data  models instead of raw data may arise. Nevertheless, model dissemination has received  very little attention so far. In this paper, we attempt to fill this gap and propose  a model-agnostic dissemination framework that can handle different models in a uniform  manner. The dissemination infrastructure is built on top of a tree-based overlay network,  reminiscent to the ones employed in publish/subscribe systems, which are known to scale  well to the number of data producers and receivers. To adequately deal with the vast  model variation and receivers' very different accuracy requirements on the models, we  have developed optimized model routing algorithms, which are intended to minimize data  traffic and avoid bottlenecks within the dissemination network. The extensive experimental  evaluation over a prototype system that we have built shows that our methods are both  effective and robust.</abstract>
	</article>
	<article>
		<articleId>461</articleId>
		<title>Efficient Subgraph Search over Large Uncertain Graphs</title>
		<trackName>Research</trackName>
		<abstract>Retrieving graphs containing a query graph from a large graph database is a key task in many graph-based applications, including chemical compounds discovery, protein complex prediction, and structural pattern recognition. However, graph data handled by these applications is often noisy, incomplete, and inaccurate because of the way the data is produced. In this paper,we study subgraph queries over uncertain graphs. Specifically, we consider the problem  of answering threshold-based probabilistic queries over a large  uncertain graph database with the possible world semantics. We   prove that problem is #P-complete, therefore, we adopt a filtering-and-verification strategy to speed up the   search. In the filtering phase, we use a probabilistic   inverted index, PIndex, based on subgraph features obtained   by an optimal feature selection process. During the verification   phase, we develop exact and bound algorithms to validate the   remaining candidates. Extensive experimental results demonstrate the   effectiveness of the proposed algorithms.</abstract>
	</article>
	<article>
		<articleId>468</articleId>
		<title>Entangled Transactions</title>
		<trackName>Research</trackName>
		<abstract>As the world becomes more interdependent and computing grows more collaborative, there is a need for new abstractions and tools to help users work together. We recently introduced entangled queries – a mechanism for information exchange between database queries. In this paper, we introduce entangled transactions, units of work similar to traditional transactions that however do not run in isolation, but communicate with each other via entangled queries.  Supporting entangled transactions brings about many new challanges, from an abstract model to an investigation of the unique systems issues that arise during their implementation. We first introduce a novel semantic model for entangled transactions that comes with analogues of the classical ACID properties. We then discuss execution models for entangled transactions and select a concrete design motivated by application scenarios. With a prototype system that implements this design, we show experimental results that demonstrate the viability of entangled transactions in real-world application settings.</abstract>
	</article>
	<article>
		<articleId>471</articleId>
		<title>Summary Graphs for Relational Database Schemas</title>
		<trackName>Research</trackName>
		<abstract>Increasingly complex databases require ever more sophisticated tools to help users understand their schemas and interact with the data. Existing tools fall short of either providing the ``big picture,'' or of presenting useful connectivity information.  In this paper we define summary graphs, a novel approach for summarizing schemas. Given a set of user-specified query tables, the summary graph automatically computes the most relevant tables and joins {\em for that query set}. The output preserves the most informative join paths between the query tables, while meeting size constraints. In the process, we define a novel information-theoretic measure over join edges. Unlike most subgraph extraction work, we allow metaedges (i.e., edges in the transitive closure) to help reduce output complexity. We prove that the problem is NP-Hard, and solve it as an integer program. Our extensive experimental study shows that our method returns high-quality summaries under independent quality measures. </abstract>
	</article>
	<article>
		<articleId>476</articleId>
		<title>RecBench: Benchmarks for Evaluating Performance of Recommender System Architectures</title>
		<trackName>Research</trackName>
		<abstract>Traditionally, recommender systems have been "hand-built", implemented as custom applications hard-wired to a particular recommendation task. Recently, the database community has begun exploring alternative DBMS-based recommender system architectures, whereby a database both stores the recommender system data (e.g., ratings data and the derived recommender models) and generates recommendations using SQL queries. In this paper, we present a comprehensive experimental comparison of both architectures. We define a set of benchmark tasks based on the needs of a typical recommender-powered e-commerce site. We then evaluate the performance of the "hand-built" MultiLens recommender application against two DBMS-based implementations: an unmodified DBMS and RecStore, a DBMS modified to improve efficiency in incremental recommender model updates. We employ two non-trivial data sets in our study: the 10 million rating MovieLens data, and the 100 million rating data set used in the Netflix Challenge. This study is the first of its kind, and our findings reveal an interesting trade-off: "hand-built" recommenders exhibit superior performance in model-building and pure recommendation tasks, while DBMS-based recommenders are superior at more complex recommendation tasks such as providing filtered recommendations and blending text-search with recommendation prediction scores.</abstract>
	</article>
	<article>
		<articleId>480</articleId>
		<title>Business Policy Modeling and Enforcement in Databases</title>
		<trackName>Research</trackName>
		<abstract>Database systems are the central information repositories for businesses and are subject to a wide array of policies, rules and requirements. The spectrum of business level constraints implemented within database systems has expanded from classical access control to include auditing, usage control, privacy management, and records retention. The lack of a systematic mechanism of integrating and reasoning about such a diverse set of policies manifested as database level constraints makes corporate policy management a chaotic process.  In this paper we propose a general purpose policy modeling and constraint management framework that can integrate numerous aspects of business level requirements within database systems. Our proposed solution relies on a finite state modeling language for business level policies, in which users can declaratively express rules related to the normal workflow of a business process as well as specifying any undesirable states of business objects contained in a database system. The proposed system is then able to translate these policies into low level temporal integrity constraints that prevent policy violations and ensure that business objects and artifacts follow their mandated lifecycles. A formal layer for reasoning allows policy makers to discover unenforceable and conflicting policies, providing the basis to guarantee compliance for a wide array of rules that may need to be enforced on complex business objects stored in relational database systems.</abstract>
	</article>
	<article>
		<articleId>494</articleId>
		<title>Online Data Fusion</title>
		<trackName>Research</trackName>
		<abstract>The Web contains a significant volume of structured data in various domains, but meanwhile, a fair volume of Web data are dirty and erroneous, and such data can get quickly populated through copying between Web sources. While advanced data integration techniques allow querying structured data on the Web, they return the union of the answers retrieved from different sources and can thus return conflicting information, leaving decisions of which answers are correct to the end users. Data fusion techniques, on the other hand, aim at finding the true values that reflect the real world, but are designed for offline data aggregation and can take a long time. This paper proposes the first online data fusion system. It starts with returning answers from the first probed source, and refreshes the answers as it probes more sources and applies fusion techniques on the retrieved data. For each returned answer, it shows the likelihood that the answer is correct, and terminates after gaining enough confidence that data from the unprocessed sources are unlikely to change the returned answers. We address key problems in building such a system and show empirically that the system can return correct answers quickly and probe much less sources without sacrificing the quality of the answers.</abstract>
	</article>
	<article>
		<articleId>498</articleId>
		<title>Optimistic Concurrency Control by Melding Trees</title>
		<trackName>Research</trackName>
		<abstract>This paper describes a new optimistic concurrency control algorithm for tree-structured data called meld. Each transaction executes on a snapshot of a multiversion database and logs a record with its intended updates. Meld processes log records in log order on a cached partial-copy of the last committed state to determine whether each transaction commits. If so, it merges the transaction’s updates into that state. Meld is used in the Hyder transaction system and enables Hyder to scale out without partitioning. Since meld is on the critical path of transaction execution, it must be very fast. The paper describes the meld algorithm in detail and reports on an evaluation of an implementation. It can perform over 400K update transactions per second for transactions with two operations, and 130K for transactions with eight operations.</abstract>
	</article>
	<article>
		<articleId>506</articleId>
		<title>Linking Temporal Records</title>
		<trackName>Research</trackName>
		<abstract>Many data sets contain temporal records over a long period of time; each record is associated with a time stamp and describes some aspects of a realworld entity at that particular time (e.g., author information in DBLP). In such cases, we often wish to identify records that describe the same entity over time and so be able to enable interesting longitudinal data analysis. However, existing record linkage techniques ignore the temporal information and can fall short for temporal data.  This paper studies linking temporal records. First, we apply time decay to capture the effect of elapsed time on entity value evolution. Second, instead of comparing each pair of records locally, we propose clustering methods that consider time order of the records and make global decisions. Experimental results show that our algorithms significantly outperform traditional linkage methods on various temporal data sets.</abstract>
	</article>
	<article>
		<articleId>510</articleId>
		<title>Efficient Algorithms for Finding Optimal Meeting Point on Road Networks</title>
		<trackName>Research</trackName>
		<abstract>Given a set of points Q on a road network, an optimal meeting point (OMP) query returns the point on a road network G=(V, E) with the smallest sum of network distances to all the points in Q. This problem has many real world applications, such as minimizing the total travel cost for a group of people who want to find a location for gathering. While this problem has been well studied in the Euclidean space, the recently proposed state-of-the-art algorithm for solving this problem in the context of road networks is still not efficient. In this paper, we propose a new baseline algorithm for the OMP query, which reduces the search space from |Q||E| to |V|+|Q|. We also present two effective pruning techniques that further accelerate the baseline algorithm. Finally, in order to support spatial applications that involve large flow of queries and require fast response, an extremely efficient algorithm is proposed to find a high-quality near-optimal meeting point, which is orders of magnitude faster than the exact OMP algorithms. Extensive experiments are conducted to verify the efficiency of our algorithms.</abstract>
	</article>
	<article>
		<articleId>527</articleId>
		<title>Optimal Schemes for Robust Web Extraction</title>
		<trackName>Research</trackName>
		<abstract>In this paper, we consider the problem of constructing wrappers for web information extraction that are robust to changes in websites. We consider two models to study robustness formally: the adversarial model, where we look at the worst-case robustness of wrappers, and probabilistic model, where we look at the expected robustness of wrappers, as web-pages evolve. Under both models, we present optimal algorithms for constructing the most robust wrapper. By evaluating on real websites, we demonstrate that in practice, our algorithms are highly effective in coping up with changes in websites, and reduce the wrapper breakage by up to 500% over existing techniques.</abstract>
	</article>
	<article>
		<articleId>531</articleId>
		<title>PathSim: Meta Path-Based Top-K Similarity Search in Heterogeneous Information Networks</title>
		<trackName>Research</trackName>
		<abstract>Similarity search is a primitive operation in database and Web search engines. With the advent of large-scale heterogeneous information networks that consist of multi-typed, interconnected objects, such as the bibliographic networks and social media networks, it is important to study similarity search in such networks. Intuitively, two objects are similar if they are linked by many paths in the network. However, most existing similarity measures are defined for homogeneous networks. Different semantic meanings behind paths are not taken into consideration. Thus they cannot be directly applied to heterogeneous networks.  In this paper, we study similarity search that is defined among the same type of objects in heterogeneous networks. Moreover, by considering different linkage paths in a network, one could derive various similarity semantics. Therefore, we introduce the concept of meta path-based similarity, where a meta path is a path consisting of a sequence of relations defined between different object types (i.e., structural paths at the meta level). No matter whether a user would like to explicitly specify a path combination given sufficient domain knowledge, or choose the best path by experimental trials, or simply provide training examples to learn it, meta path forms a common base for a network-based similarity search engine. In particular, under the meta path framework we define a novel similarity measure called PathSim that is able to find peer objects in the network (e.g., find authors in the similar field and with similar reputation), which turns out to be more meaningful in many scenarios compared with random-walk based similarity measures. In order to support fast online query processing for PathSim queries, we develop an efficient solution that partially materializes short meta paths and then concatenates them online to compute top-k results. Experiments on real data sets demonstrate the effectiveness and efficiency of our proposed paradigm.</abstract>
	</article>
	<article>
		<articleId>535</articleId>
		<title>Optimizing Query Answering under Ontological Constraints</title>
		<trackName>Research</trackName>
		<abstract>Ontological queries are evaluated against a database combined with ontological constraints and answering such queries is a challenging new problem for database research. For many ontological modelling languages, query answering can be solved via query rewriting: given a conjunctive query and an ontology, the query can be transformed into a first-order query, called the perfect rewriting, that takes into account the semantic consequences of the ontology. Then, for every extensional database D, the answer to the query is obtained by evaluating the rewritten query against D. In this paper we present a new algorithm that computes the perfect rewriting of a conjunctive query w.r.t. a linear Datalog+- ontology. Also, we provide an experimental comparison of our algorithm with existing rewriting techniques.</abstract>
	</article>
	<article>
		<articleId>549</articleId>
		<title>OXPath: A Language for Scalable, Memory-efficient Data Extraction from Web Applications</title>
		<trackName>Research</trackName>
		<abstract>The evolution of the web has outpaced itself: The growing wealth of information and the increasing sophistication of interfaces necessitate automated processing. Web automation and extraction technologies have been overwhelmed by this very growth. To address this trend, we identify four key requirements of web extraction: (1) Interact with sophisticated web application interfaces, (2) Precisely capture the relevant data for most web extraction tasks, (3) Scale with the number of visited pages, and (4) Readily embed into existing web technologies.  We introduce OXPath, an extension of XPath for interacting with web applications and for extracting information thus revealed. It addresses all the above requirements. OXPath’s page-at-a-time evaluation guarantees memory use independent of the number of visited pages, yet remains polynomial in time. We validate experimentally the theoretical complexity and demonstrate that its evaluation is dominated by the page rendering of the underlying browser.  Our experiments show that OXPath outperforms existing commercial and academic data extraction tools by a wide margin. OXPath is available under an open source license.</abstract>
	</article>
	<article>
		<articleId>554</articleId>
		<title>Optimizing and Parallelizing Ranked Enumeration</title>
		<trackName>Research</trackName>
		<abstract>Lawler-Murty's procedure is a general tool for designing algorithms   for enumeration problems (i.e., problems that involve the production   of a large set of answers in ranked order), which naturally arise in   database management. Lawler-Murty's procedure is used in a variety of modern   database applications; particularly in those related to keyword   search over structured data. Essentially, this procedure enumerates   by invoking a series of instances of an optimization problem (i.e.,   finding the best solution); solving the optimization problem is the   only part that depends on the specific task at hand.  The topic of   optimizing and parallelizing Lawler-Murty's procedure is   investigated. Naive parallelism can be carried out by concurrently   solving independent instances of the optimization problem. This can   be improved by printing the next answer, in the enumeration order,   as soon as none of the concurrent instances can produce a better   answer.  However, this approach alone suffers from poor utilization   of available threads. That leads to the idea of freezing an instance   of the optimization problem. Interestingly, not only is freezing   beneficial to the parallel execution of Lawler-Murty's, it also   substantially reduces the running time of the serial execution.   Additional improvements of the freezing technique are then developed   to further enhance the utilization of threads, and they result in a   significant overall speedup. The effectiveness of the proposed   approach is demonstrated on keyword search over data graphs, wherein   an extensive experimental study is described. </abstract>
	</article>
	<article>
		<articleId>568</articleId>
		<title>Where in the World is My Data?</title>
		<trackName>Research</trackName>
		<abstract>Users of websites such as Facebook, Ebay and Yahoo! demand fast response times, and these sites replicate data across globally distributed  datacenters to achieve this.  However, it is not necessary to replicate all data to all locations: if a European user's record is never accessed in Asia, it does not make sense to pay the bandwidth and disk costs to maintain an Asian replica.   In this paper, we describe mechanisms for selectively replicating large-scale web databases on a record-by-record basis. We introduce a flexible constraint language to specify replication policy constraints. We then present an adaptive scheme for replicating data to where it is most frequently accessed, while respecting policy constraints and using minimal bookkeeping. Experiments using a modified version of our PNUTS system demonstrate our techniques work well.</abstract>
	</article>
	<article>
		<articleId>573</articleId>
		<title>Queries with Difference on Probabilistic Databases</title>
		<trackName>Research</trackName>
		<abstract>We study the feasibility of the exact and approximate computation of the probability of relational queries with  difference on tuple-independent databases. We show that even  the difference between two ``safe'' conjunctive queries  without self-joins is ``unsafe'' for exact computation. We  turn to approximation and design an FPRAS for a large class of relational queries with difference,  limited by how difference is nested and by the nature of the subtracted sub-queries. We give examples of inapproximable queries outside this class.</abstract>
	</article>
	<article>
		<articleId>575</articleId>
		<title>MRI: Meaningful Interpretations of Collaborative Ratings</title>
		<trackName>Research</trackName>
		<abstract>Collaborative rating sites (e.g., IMDb and CNet) have become essential resources that many users consult to make purchasing decisions on various items such as movies and products. Ideally, a user wants to quickly decide whether an item is desirable, especially when many choices are available. In practice, however, a user either spends a lot of time examining ratings and reviews before making an informed decision, or simply trusts overall rating aggregations associated with an item. In this paper, we argue that neither option is satisfactory and propose a novel and powerful third option, Meaningful Ratings Interpretation (MRI), that automatically provides a meaningful interpretation of ratings associated with the input items and aids a user make informed decision with minimal effort. As a simple example, given the movie "Usual Suspects," instead of simply showing the average rating of 8.7 from all reviewers, MRI produces a set of meaningful factoids such as "male reviewers under 30 from NYC love this movie" and "female reviewers over 60 from Texas do not like this movie." We define the notion of meaningful interpretation based on the idea of data cube, and formalize two important sub-problems, meaningful description mining and meaningful difference mining. We show that these problems are NP-hard and design novel randomized hill exploration algorithms to solve them efficiently. We conduct user studies to show that MRI provides more helpful information to the users than simple average ratings. Performance evaluation over real data shows that our algorithms perform much faster and generate equally good interpretations as brute-force algorithms.</abstract>
	</article>
	<article>
		<articleId>577</articleId>
		<title>Storing Matrices on Disk: Theory and Practice Revisited</title>
		<trackName>Research</trackName>
		<abstract>We consider the problem of storing arrays on disk to support scalable data analysis involving linear algebra. We propose Linearized Array B-tree, or LAB-tree, which supports flexible array layouts and automatically adapts to varying sparsity across parts of an array and over time. We reexamine the B-tree splitting strategy for handling insertions and the flushing policy for batching updates, and show that common practices may in fact be suboptimal. Through theoretical and empirical studies, we propose alternatives with good theoretical guarantees and/or practical performance.</abstract>
	</article>
	<article>
		<articleId>578</articleId>
		<title>Publishing Set-Valued Data via Differential Privacy</title>
		<trackName>Research</trackName>
		<abstract>Set-valued data provides enormous opportunities for various data mining tasks. In this paper, we study the problem of publishing set-valued data for data mining tasks under the rigorous differential privacy model. All existing data publishing methods for set-valued data are based on partition-based privacy models, for example $k$-anonymity, which are vulnerable to privacy attacks based on background knowledge. In contrast, differential privacy provides strong privacy guarantees independent of an adversary's background knowledge and computational power. Existing data publishing approaches for differential privacy, however, are not adequate in terms of both utility and scalability in the context of set-valued data due to its high dimensionality.  We demonstrate that set-valued data could be efficiently released under differential privacy with guaranteed utility with the help of context-free taxonomy trees. We propose a probabilistic top-down partitioning algorithm to generate a differentially private release, which scales linearly with the input data size. We also discuss the applicability of our idea to the context of relational data. We prove that our result is $(\epsilon, \delta)$-useful for the class of counting queries, the foundation of many data mining tasks. We show that our approach maintains high utility for counting queries and frequent itemset mining and scales to large datasets through extensive experiments on real-life set-valued datasets.</abstract>
	</article>
	<article>
		<articleId>589</articleId>
		<title>Randomized Generalization for Aggregate Suppression over Hidden Web Databases</title>
		<trackName>Research</trackName>
		<abstract>Many web databases are hidden behind restrictive form-like interfaces which allow users to execute search queries over the underlying hidden database. While it is important to support such search queries, many hidden database owners also want to maintain a certain level of privacy for aggregate information over their databases, for reasons including business secrets and homeland security. Existing work on aggregate suppression thwarts the uniform random sampling of a hidden database, but cannot address recently proposed attacks which accurately estimate SUM and COUNT queries without the need to first draw a uniform random sample. In this paper, we consider the problem of suppressing SUM and COUNT queries over a hidden database. In particular, we develop randomized generalization, a novel technique which provides rigid aggregate-suppression guarantee while maintaining the utility of individual search queries. We present theoretical analysis and extensive experiments to illustrate the effectiveness of our approach.</abstract>
	</article>
	<article>
		<articleId>591</articleId>
		<title>Profiling, What-if Analysis, and Cost-based Optimization of MapReduce Programs</title>
		<trackName>Research</trackName>
		<abstract>MapReduce has emerged as a viable competitor to database systems in big data analytics. MapReduce programs are being written  for a wide variety of application domains including  business data processing, text analysis, natural language processing,  Web graph and social network analysis, and computational science.  However, MapReduce systems lack a feature that has been key to  the historical success of database systems, namely,  cost-based optimization. A major challenge here is that, to the MapReduce system, a program consists of black-box map and reduce functions written in some  programming language like C++, Java, Python, or Ruby. We introduce,  to our knowledge, the first Cost-based Optimizer   for simple to arbitrarily complex MapReduce programs.   We focus on the optimization opportunities   presented by the large space of configuration parameters for these programs. We also introduce a Profiler to collect detailed statistical information from unmodified MapReduce programs, and  a What-if Engine for fine-grained cost estimation. All components have been prototyped for the popular Hadoop MapReduce system. The effectiveness of each component is demonstrated through a comprehensive evaluation using representative MapReduce programs from various application domains. </abstract>
	</article>
	<article>
		<articleId>599</articleId>
		<title>Scalable SPARQL Querying of Large RDF Graphs</title>
		<trackName>Research</trackName>
		<abstract>The generation of RDF data has accelerated to the point where many data sets need to be partitioned across multiple machines in order to achieve reasonable performance when querying the data. Although tremendous progress has been made in the Semantic Web community for achieving high performance data management on a single node, current solutions that allow the data to be partitioned across multiple machines are highly inefficient. In this paper, we introduce a scalable RDF data management system that is up to three orders of magnitude more efficient than popular multi-node RDF data management systems. In so doing, we introduce techniques for (1) leveraging state-of-the-art single node RDF-store technology (2) partitioning the data across nodes in a manner that helps accelerate query processing through locality optimizations and (3) decomposing SPARQL queries into high performance fragments that take advantage of how data is partitioned in a cluster.</abstract>
	</article>
	<article>
		<articleId>601</articleId>
		<title>Online Aggregation for Large MapReduce Jobs</title>
		<trackName>Research</trackName>
		<abstract>In online aggregation, a database system processes a user’s aggregation query in an online fashion. At all times during processing, the system gives the user an estimate of the final query result, with the confidence bounds that become tighter over time. In this paper, we consider how online aggregation can be built into a MapReduce system for large-scale data processing. Given the MapReduce paradigm’s close relationship with cloud computing (in that one might expect a large fraction of MapReduce jobs to be run in the cloud), online aggregation is a very attractive technology. Since large-scale cloud computations are typically pay-as-you-go, a user can monitor the accuracy obtained in an online fashion, and then save money by killing the computation early once sufficient accuracy has been obtained.</abstract>
	</article>
	<article>
		<articleId>602</articleId>
		<title>Private Analysis of Graph Structure</title>
		<trackName>Research</trackName>
		<abstract> We present efficient algorithms for releasing useful statistics about graph data while providing rigorous privacy guarantees. Our algorithms work on data sets that consist of relationships between individuals, such as social ties or email communication. The algorithms satisfy edge differential privacy, which essentially requires that the presence or absence of any particular relationship be hidden.  Our algorithms output approximate answers to subgraph counting queries. Given a query graph H, e.g., a triangle, k-star or k-triangle, the goal is to return the number of edge-induced isomorphic copies of H in the input graph. The special case of triangles was considered by Nissim, Raskhodnikova and Smith (STOC 2007), and a more general investigation of arbitrary query graphs was initiated by Rastogi, Hay, Miklau and Suciu (PODS 2009). We extend the approach of [NRS] to a new class of statistics, namely, k-star queries. We also give algorithms for k-triangle queries using a different approach, based on the higher-order local sensitivity.  For the specific graph statistics we consider (i.e., k-stars and k-triangles), we significantly improve on the work of [RHMS]: our algorithms satisfy a stronger notion of privacy, which does not rely on the adversary having a particular prior distribution on the data, and add less noise to the answers before releasing them.  We evaluate the accuracy of our algorithms both theoretically and empirically, using a variety of real and synthetic data sets.  We give explicit, simple conditions under which these algorithms add a small amount of noise. We also provide the average-case analysis in the Erdos-Renyi-Gilbert G(n,p) random graph model.  Finally, we give hardness results indicating that the approach NRS used for triangles cannot easily be extended to k-triangles (and hence justifying our development of a  new algorithmic approach). </abstract>
	</article>
	<article>
		<articleId>614</articleId>
		<title>Stratification Criteria and Rewriting Techniques for Checking Chase Termination</title>
		<trackName>Research</trackName>
		<abstract>The Chase is a fixpoint algorithm enforcing satisfaction of data dependencies in databases. Its execution involves the insertion of tuples with possible null values and the changing of null values which can be made equal to constants or other null values. Since the chase fixpoint evaluation could be on-terminating, in recent years the problem know as chase termination has been investigated. It consists in the detection of sufficient conditions, derived from the structural analysis of dependencies, guaranteeing that the chase fixpoint terminates independently from the database instance. Several criteria introducing sufficient conditions for chase termination have been recently proposed \cite{FaginTCS,Deutsch-08,Lausen-09,Marnette-09}.  The aim of this paper is to present more general criteria and techniques for chase termination. We first present extensions of the well-know stratification criterium and introduce a new criterium, called local stratification (LS), which generalizes both super-weak acyclicity and stratification-based criteria (including the class of constraints which are inductively restricted). Next the paper presents a rewriting algorithm, whose structure is similar to the one presented in \cite{GreSpe10}; the algorithm takes as input a set of tuple generating dependencies  and produces as output an equivalent set of dependencies and a boolean value stating whether a sort of cyclicity has been detected. The output set, obtained by adorning the input set of constraints, allows us to perform a more accurate analysis of the structural properties of constraints and to further enlarge the class of tuple generating dependencies for which chase termination is guaranteed, whereas the checking of acyclicity allows us to introduce the class of acyclic constraints (AC), which generalizes LS and guarantees chase termination.</abstract>
	</article>
	<article>
		<articleId>618</articleId>
		<title>Optimizing Probabilistic Query Processing on Continuous Uncertain Data</title>
		<trackName>Research</trackName>
		<abstract>Uncertain data management is becoming increasingly important in many applications, in particular, in scientific databases and data stream systems. Uncertain data in these new environments is naturally modeled by continuous random variables. An important class of queries uses complex selection and join predicates and requires query answers to be returned if their existence probabilities pass a threshold. In this work, we optimize threshold query processing for continuous uncertain data by (i) expediting  joins using new indexes on uncertain data, (ii) expediting selections by reducing dimensionality of integration and using  faster filters, and (iii) optimizing a query plan using a dynamic, per-tuple based approach. Evaluation results using real-world data and benchmark queries show the accuracy and efficiency of our techniques and significant performance gains over  a state-of-the-art threshold query optimizer. </abstract>
	</article>
	<article>
		<articleId>621</articleId>
		<title>Massive Scale-out of Expensive Continuous Queries</title>
		<trackName>Research</trackName>
		<abstract>Scalable execution of expensive continuous queries over massive data streams requires input streams to be split into parallel sub-streams. The query operators are continuously executed in parallel over these sub-streams. Stream splitting involves both partitioning and replication of incoming tuples, depending on how the continuous query is parallelized. We provide a stream splitting operator that enables such customized stream splitting. However, it is critical that the stream splitting itself keeps up with input streams of high volume. This is a problem when the stream splitting predicates have some costs. Therefore, to enable customized splitting of high-volume streams, we introduce a parallelized stream splitting operator, called parasplit. We investigate the performance of parasplit using a cost model and experimentally. Based on these results, a heuristic is devised to automatically parallelize the execution of parasplit. We show that the maximum stream rate of parasplit is network bound, and that the parallelization is energy efficient. Finally, the scalability of our approach is experimentally demonstrated on the Linear Road Benchmark, showing an order of magnitude higher stream processing rate over previously published results, allowing at least 512 expressways.</abstract>
	</article>
	<article>
		<articleId>633</articleId>
		<title>Keyword Search on Form Results</title>
		<trackName>Research</trackName>
		<abstract>In recent years there has been a good deal of research in the area of keyword search on structured and semi-structured data.  Most of this body of work has a significant limitation in the context of enterprise data since it ignores the application code that has often been carefully designed to present data in a meaningful fashion to users.  In this work, we consider how to perform keyword search on enterprise applications, which provide a number of forms that can take parameters; parameters may be explicit, or implicit such as the identifier of the user.  In the context of such applications, the goal of keyword search is, given a set of keywords, to retrieve forms, along with parameter values, such that result of each retrieved form executed on the corresponding retrieved parameter values will contain the specified keywords.  Some earlier work in this area was based on creating keyword indices on form results, but there are problems in maintaining such indices in the face of updates.  In contrast, we propose techniques based on creating inverted SQL queries from the SQL queries in the forms. Unlike earlier work, our techniques do not require any special purpose indices, and instead make use of standard text indices supported by most database systems.  We have implemented our techniques and show that keyword search can run at reasonable speeds even on large databases with a significant number of forms. </abstract>
	</article>
	<article>
		<articleId>655</articleId>
		<title>Efficient Rank Join with Aggregation Constraints</title>
		<trackName>Research</trackName>
		<abstract>We show aggregation constraints that naturally arise in several applications  can enrich the semantics of rank join queries, by allowing users to impose their application-specific preferences in a declarative way. By analyzing the properties of aggregation constraints, we develop efficient deterministic and probabilistic algorithms which can push the aggregation constraints inside the rank join framework.  Through extensive experiments on various datasets, we show that in many cases our proposed algorithms can significantly outperform the naive approach of applying the state-of-the-art rank join algorithm followed by post-filtering to discard results violating the constraints. </abstract>
	</article>
	<article>
		<articleId>134</articleId>
		<title>Recovering Semantics of Tables on the Web </title>
		<trackName>Research</trackName>
		<abstract>The Web offers a corpus of over 100 million tables, but the meaning of each table is rarely explicit from the table itself. Header rows exist in few cases and even when they do, the attribute names are typically useless. We describe a system that attempts to recover the semantics of tables by enriching the table with additional annotations. Our annotations facilitate operations such as searching for tables and finding related tables.  To recover semantics of tables, we leverage a database of class labels and relationships automatically extracted from the Web. The database of classes and relationships has very wide coverage, but is also noisy. We attach a class label to a column if a sufficient number of the values in the column are identified with that label in the database of class labels, and analogously for binary relationships. We describe a formal model for reasoning about when we have seen sufficient evidence for a label, and show that it performs substantially better than a simple majority scheme. We describe a set of experiments that illustrate the utility of the recovered semantics for table search and show that it performs substantially better than previous approaches. In addition, we characterize what fraction of tables on the Web can be annotated using our approach. </abstract>
	</article>
	<article>
		<articleId>137</articleId>
		<title>Tuffy: Scaling up Statistical Inference in Markov Logic Networks using an RDBMS</title>
		<trackName>Research</trackName>
		<abstract>Markov Logic Networks (MLNs) have emerged as a powerful framework that combines statistical and logical reasoning; they have been applied to many data intensive problems including information extraction, entity resolution, and text mining. Current implementations of MLNs do not scale to large real-world data sets, which is preventing their wide-spread adoption. We present Tuffy that achieves scalability via three novel contributions: (1) a bottom-up approach to grounding that allows us to leverage the full power of the relational optimizer, (2) a novel hybrid architecture that allows us to perform AI-style local search efficiently using an RDBMS, and (3) a theoretical insight that shows when one can (exponentially) improve the efficiency of stochastic local search. We leverage (3) to build novel partitioning, loading, and parallel algorithms. We show that our approach outperforms state-of-the-art implementations in both quality and speed on several publicly available datasets. </abstract>
	</article>
	<article>
		<articleId>139</articleId>
		<title>Incrementally Maintaining Classification using an RDBMS</title>
		<trackName>Research</trackName>
		<abstract>The proliferation of imprecise data has motivated both researchers and the database industry to push statistical techniques into relational database management systems (RDBMSs). We study algorithms to maintain model-based views for a popular statistical technique, classification, inside an RDBMS in the presence of updates to the training examples. We make three technical contributions: (1) An algorithm that incrementally maintains classification inside an RDBMS. (2) An analysis of the above algorithm that shows that our algorithm is optimal among all deterministic algorithms (and asymptotically within a factor of 2 of a nondeterministic optimal). (3) An index structure based on the technical ideas that underlie the above algorithm which allows us to store only a fraction of the entities in memory. We apply our techniques to text processing, and we demonstrate that our algorithms provide several orders of magnitude improvement over non-incremental approaches to classification on a variety of data sets: such as the Cora, UCI Machine Learning Repository data sets, Citeseer, and DBLife.</abstract>
	</article>
	<article>
		<articleId>145</articleId>
		<title>High-Throughput Transaction Executions on Graphics Processors</title>
		<trackName>Research</trackName>
		<abstract>OLTP (On-Line Transaction Processing) is an important business system sector in various traditional and emerging online services. Due to the increasing number of users, OLTP systems require high throughput for executing tens of thousands of transactions in a short time period. Encouraged by the recent success of GPGPU (General-Purpose computation on Graphics Processors), we propose GPUTx, an OLTP engine performing high-throughput transaction executions on the GPU for in-memory databases. Compared with existing GPGPU studies usually optimizing a single task, transaction executions require handling many small tasks concurrently. Specifically, we propose the bulk execution model to group multiple transactions into a bulk and to execute the bulk on the GPU as a single task. The transactions within the bulk are executed concurrently on the GPU. We study three basic execution strategies (one with locks and the other two lock-free), and optimize them with the GPU features including the hardware support of atomic operations, the massive thread parallelism and the SPMD (Single Program Multiple Data) execution. We evaluate GPUTx on a recent NVIDIA GPU in comparison with its counterpart on a quad-core CPU. Our experimental results show that optimizations on GPUTx significantly improve the throughput, and the optimized GPUTx achieves 4-10 times higher throughput than its CPU-based counterpart on public transaction processing benchmarks.</abstract>
	</article>
	<article>
		<articleId>148</articleId>
		<title>Distributed Inference and Query Processing for RFID Tracking and Monitoring</title>
		<trackName>Research</trackName>
		<abstract>In this paper, we present the design of a scalable, distributed stream processing system for RFID tracking and monitoring. Since RFID data lacks containment and location information that is key to query processing, we propose to combine location and containment inference with stream query processing in a single architecture, with inference as an enabling mechanism for high-level query processing. We further consider challenges in instantiating such a system in large distributed settings and design techniques for distributed inference and query processing. Our experimental results, using both real-world data and large synthetic traces, demonstrate the accuracy, efficiency, and scalability of our proposed techniques.</abstract>
	</article>
	<article>
		<articleId>157</articleId>
		<title>Synthesizing Products for Online Catalogs</title>
		<trackName>Research</trackName>
		<abstract>A high-quality, comprehensive product catalog is essential to the success of Product Search engines and shopping sites such as Yahoo! Shopping, Google Product Search, and Bing Shopping. Given the large number of products and the speed at which they are released to the market, keeping catalogs up-to-date becomes a challenging task, calling for the need of automated techniques. In this paper, we introduce the problem of product synthesis, a key component of catalog creation and maintenance. Given a set of offers advertised by merchants, the goal is to identify new products and add them to the catalog, together with their (structured) attributes. A fundamental challenge in product synthesis is the scale of the problem: a Product Search engine receives data from thousands of merchants and millions of products; the product taxonomy contains thousands of categories, where each category has a different schema; and merchants use representations for products that are different from the ones used in the catalog of the Product Search engine.  We propose a system that provides an end-to-end solution to the product synthesis problem, and includes components for extraction, and addresses issues involved in data extraction from offers, schema reconciliation, and data fusion. We developed a novel and scalable technique for schema matching which leverages knowledge about previously-known instance-level associations between offers and products; and it is trained using automatically created training sets (no manually-labeled data is needed). We present an experimental evaluation of our system using data from Bing Shopping for more than 800K offers, a thousand merchants, and 400 categories. The evaluation confirms that our approach is able to automatically generate a large number of accurate product specifications. Furthermore, the evaluation shows that our schema reconciliation component outperforms state-of-the-art schema matching techniques in terms of precision and recall.</abstract>
	</article>
	<article>
		<articleId>160</articleId>
		<title>On Pruning for Top-K Ranking in Uncertain Databases</title>
		<trackName>Research</trackName>
		<abstract>Top-k ranking for an uncertain database is to rank tuples in it so that the best k of them can be determined. The problem has been formalized under the unified approach based on parameterized ranking functions (PRFs) and the possible world semantics. Given a PRF, one can always compute the ranking function values of all the tuples to determine the top-k tuples, which is a formidable task for large databases. In this paper, we present a general approach to pruning for the framework based on PRFs. We show a mathematical manipulation of possible worlds which reveals key insights in the part of computation that may be pruned and how to achieve it in a systematic fashion. This leads to concrete pruning methods for a wide  range of ranking functions. We show experimentally the effectiveness of our approach.</abstract>
	</article>
	<article>
		<articleId>172</articleId>
		<title>Efficient Parallel Lists Intersection and Index Compression Algorithms using Graphics Processing Units</title>
		<trackName>Research</trackName>
		<abstract>Major web search engines answer thousands of queries per second requesting information about billions of web pages. The data sizes and query loads are growing at an exponential rate. To manage the heavy workload, we consider techniques for utilizing a Graphics Processing Unit (GPU). We investigate new approaches to improve two important operations of search engines - lists intersection and index compression. For lists intersection, we develop techniques for efficient implementation of the binary search algorithm for parallel computation. We inspect some representative real-world datasets and find that a sufficiently long inverted list has an overall linear rate of increase. Based on this observation, we propose Linear Regression and Hash Segmentation techniques for contracting the search range. For index compression, the traditional d-gap based compression schemata are not well-suited for parallel computation, so we propose a Linear Regression Compression schema which has an inherent parallel structure. We further discuss how to efficiently intersect the compressed lists on a GPU. Our experimental results show significant improvements in the query processing throughput on several datasets.</abstract>
	</article>
	<article>
		<articleId>38</articleId>
		<title>Graph Indexing of Road Networks for Shortest Path Queries with Label Restrictions</title>
		<trackName>Research</trackName>
		<abstract>The current widespread use of location-based services and GPS technologies has revived interest in very fast and scalable shortest path queries. We introduce a new shortest path query type in which dynamic constraints may be placed on the allowable set of edges that can appear on a valid shortest path (e.g., dynamically restricting the type of roads or modes of travel which may be considered in a multimodal transportation network). We formalize this problem as a specific variant of formal language constrained shortest path problems, which we call the Kleene Language Constrained Shortest Paths problem. To efficiently support this type of dynamically constrained shortest path query for large-scale datasets, we extend the hierarchical graph indexing technique known as Contraction Hierarchies. Our experimental evaluation using the North American road network dataset (with over 50 million edges) shows an average query speed and search space improvement of over 3 orders of magnitude compared to the naive adaptation of the standard Dijkstra’s algorithm to support this query type. We also show an improvement of over 2 orders of magnitude compared to the only previously-existing indexing technique which could solve this problem without additional preprocessing.</abstract>
	</article>
	<article>
		<articleId>45</articleId>
		<title>CRIUS: User-Friendly Database Design</title>
		<trackName>Research</trackName>
		<abstract>Non-technical users are increasingly adding structures to their data. This gives rise to the need for database design. However, traditional database design is deliberate and heavy-weight, requiring technical expertise that everyday users may not possess. For this reason, we propose that users of personal data management applications should be able to create and refine data structures in an ad-hoc way over time, thereby "organically" growing their schemas. For this purpose, we develop a spreadsheet-like direct manipulation interface. We show how integrity constraints can still provide value, even in this scenario of frequent schema and data modifications. We also develop a back-end database implementation to support this interface, with a design that permits schema changes at a low cost.  We have folded these ideas into a system, called CRIUS, which supports a nested data model and a graphical user interface. From the user's perspective, the chief advantages of CRIUS are its support for simple schema definition and modification through an intuitive drag-and-drop interface, as well as its guidance towards user data entry based on incrementally updated data integrity. We have evaluated CRIUS by means of user studies and performance studies. The user studies indicate that 1) CRIUS makes it much easier for users to design a database, as compared to state-of-the-art GUI database design tools, and 2) CRIUS makes user data entry more efficient and less error-prone. The performance experiments show that 1) the incremental integrity update in CRIUS is very efficient, making the data entry guidance applicable and 2) the back-end database implementation in CRIUS significantly improves the performance of schema update tasks, without a significant impact on other operations.</abstract>
	</article>
	<article>
		<articleId>49</articleId>
		<title>Efficient Processing of Top-k Spatial Preference Queries</title>
		<trackName>Research</trackName>
		<abstract>Top-k spatial preference queries return a ranked set of the k best data objects based on the scores of feature objects in their spatial neighborhood. Despite the wide range of location-based applications that rely on spatial preference queries, existing algorithms incur non-negligible processing cost resulting in high response time. The reason is that computing the score of any data object requires examining its spatial neighborhood to find the feature object with highest score. In this paper, we propose a novel technique to speed up the performance of top-k spatial preference queries. To this end, we propose a mapping of pairs of data and feature objects to a distance-score space, which in turn allows us to identify and materialize the minimal subset of pairs that is sufficient to answer a spatial preference query. Furthermore, we present a novel algorithm that improves query processing performance by avoiding examining the spatial neighborhood of the data objects during query execution. In addition, we propose an efficient algorithm for materialization and we describe useful properties that reduce the cost of maintenance. We show through extensive experiments that our approach significantly reduces the number of I/Os and execution time compared to the state-of-the-art algorithms for different setups.</abstract>
	</article>
	<article>
		<articleId>52</articleId>
		<title>A Probabilistic Approach for Automatically Filling Form-Based Web Interfaces</title>
		<trackName>Research</trackName>
		<abstract>In this paper we present a proposal for the implementation and evaluation of a novel method for automatically using data-rich  text for filling form-based input interfaces. Our solution takes a text as input, extracts implicit data values from it and fills appropriate fields. For this task, we rely on knowledge obtained from values of previous submissions for each field, which are freely obtained from the usage of the interfaces.  Our approach, called iForm, exploits features related to the content and the style of these values, which are combined through a Bayesian framework. Through extensive experimentation, we show that our approach is feasible and effective, and that it works well even when only a few previous submissions to the input interface are available.</abstract>
	</article>
	<article>
		<articleId>54</articleId>
		<title>HYRISE - A Main Memory Hybrid Storage Engine</title>
		<trackName>Research</trackName>
		<abstract>In this paper, we describe a main memory hybrid database system called HYRISE, which automatically partitions tables into vertical partitions of varying widths depending on how the columns of the table are accessed.  For columns accessed as a part of analytical queries (e.g., via sequential scans), narrow partitions perform better, because, when scanning a single column, cache locality is improved if the values of that column are stored contiguously.  In contrast, for columns accessed as a part of OLTP-style queries, wider partitions perform better, because such transactions frequently insert, delete, update, or access many of  the fields of a row, and co-locating those fields leads to better cache  locality.  Using a highly accurate model of cache misses, HYRISE is  able to predict the performance of different partitionings, and to automatically select the best partitioning using  an automated database design algorithm.  We show that, on a realistic  workload derived from customer applications, HYRISE can achieve a  20% to 400% performance improvement over pure all-column or  all-row designs, and that it is both more scalable and produces  better designs than previous vertical partitioning approaches for  main memory systems.</abstract>
	</article>
	<article>
		<articleId>63</articleId>
		<title>Output URL Bidding</title>
		<trackName>Research</trackName>
		<abstract>Output URL bidding is a new bidding mechanism for sponsored search,   where advertisers bid on search result URLs, as opposed to keywords   in the input query.  For example, an advertiser may want his ad to   appear whenever the search result includes the sites www.imdb.com and en.wikipedia.org, instead   of bidding on keywords that lead to these sites, e.g., movie titles   or actor names.    In this paper we study the tradeoff between the simplicity and the   specification power of output bids and we explore their utility for   advertisers. We first present a model to derive output bids from   existing keyword bids. Then, we use the derived bids to   experimentally study output bids and contrast them to input query   bids.  Our main results are the following: (1) Compact output bids   that mix both URLs and hosts have the same specification power as   more lengthy input bids; (2) Output bidding can increase the recall   of relevant queries; and (3) Output and input biding can be combined   into a hybrid mechanism that combines the benefits of both.</abstract>
	</article>
	<article>
		<articleId>74</articleId>
		<title>Human-Assisted Graph Search: It's Okay to Ask Questions</title>
		<trackName>Research</trackName>
		<abstract>We consider the problem of human-assisted graph search: given a directed acyclic graph with some (unknown) target node(s), we consider the problem of finding the target node(s) by asking an omniscient human questions of the form "Is there a target node that is reachable from the current node?''. This general problem has applications in many domains that can utilize human intelligence, including curation of hierarchies, debugging workflows, image segmentation and categorization, interactive search and filter synthesis. To our knowledge, this work provides the first formal algorithmic study of the optimization of human computation for this problem. We study various dimensions of the problem space, providing algorithms and complexity results. Our framework and algorithms can be used in the design of an optimizer for crowd-sourcing platforms such as Mechanical Turk. </abstract>
	</article>
	<article>
		<articleId>75</articleId>
		<title>Fast Sparse Matrix-Vector Multiplication on GPUs: Implications for Graph Mining</title>
		<trackName>Research</trackName>
		<abstract>Scaling up the sparse matrix-vector multiplication kernel on modern  Graphics Processing Units (GPU) has been at the heart of numerous  studies in both academia and industry. In this article we present a  novel non-parametric, self-tunable, approach to data representation  for computing this kernel, particularly targeting sparse matrices  representing power-law graphs. Using real data, we show how our  representation scheme, coupled with a novel tiling algorithm,  can yield significant benefits over the current state of the art GPU  efforts on a number of core data mining algorithms such as PageRank,  HITS and Random Walk with Restart. </abstract>
	</article>
	<article>
		<articleId>77</articleId>
		<title>Fast Incremental and Personalized PageRank</title>
		<trackName>Research</trackName>
		<abstract>In this paper, we analyze the efficiency of Monte Carlo methods for incremental computation of PageRank, personalized PageRank, and similar random walk based methods (with focus on SALSA), on large-scale dynamically evolving social networks. We assume that the graph of friendships is stored in distributed shared memory, as is the case for large social networks such as Twitter.  For global PageRank, we assume that the social network has $n$ nodes, and $m$ adversarially chosen edges arrive in a random order. We show that with a reset probability of $\epsilon$, the expected total work needed to maintain an accurate estimate (using the Monte Carlo method) of the PageRank of every node at all times is $O(\frac{n\ln m}{\epsilon^{2}})$. This is significantly better than all known bounds for incremental PageRank. For instance, if we naively recompute the PageRanks as each edge arrives, the simple power iteration method needs $\Omega(\frac{m^2}{\ln(1/(1-\epsilon))})$ total time and the Monte Carlo method needs $O(mn/\epsilon)$ total time; both are prohibitively expensive. Furthermore, we also show that we can handle deletions equally efficiently.  We then study the computation of the top $k$ personalized PageRanks starting from a seed node, assuming that personalized PageRanks follow a power-law with exponent $\alpha &lt; 1$. We show that if we store $R&gt;q\ln n$ random walks starting from every node for large enough constant $q$ (using the approach outlined for global PageRank), then the expected number of calls made to the distributed social network database is $O(k/(R^{(1-\alpha)/\alpha}))$.  We also present experimental results from the social networking site, Twitter, verifying our assumptions and analyses. The overall result is that this algorithm is fast enough for real-time queries over a dynamic social network.</abstract>
	</article>
	<article>
		<articleId>80</articleId>
		<title>Automatic Wrappers for Large Scale Web Extraction</title>
		<trackName>Research</trackName>
		<abstract>We present a generic framework to make wrapper induction algorithms tolerant to noise in the training data. This enables us to learn wrappers in a completely unsupervised manner from automatically and cheaply obtained noisy training data, e.g., using dictionaries and regular expressions. By removing the site-level supervision that wrapper-based techniques require, we are able to perform information extraction at web-scale, with accuracy unattained with existing unsupervised extraction techniques. Our system is used in production at Yahoo! and powers live applications.</abstract>
	</article>
	<article>
		<articleId>86</articleId>
		<title>Similarity Join Size Estimation using Locality Sensitive Hashing</title>
		<trackName>Research</trackName>
		<abstract>Similarity joins are important operations with a broad range of applications. In this paper, we study the problem of vector similarity join size estimation (VSJ). It is a generalization of the previously studied set similarity join size estimation (SSJ) problem and can handle more interesting cases such as TF-IDF vectors. One of the key challenges in similarity join size estimation is that the join size can change dramatically depending on the input similarity threshold.   We propose a sampling based algorithm that uses the Locality-Sensitive-Hashing (LSH) scheme. The proposed algorithm LSH-SS uses an LSH index to enable effective sampling even at high thresholds. We compare the proposed technique with random sampling and the state-of-the-art technique for SSJ (adapted to VSJ) and demonstrate LSH-SS offers more accurate estimates at both high and low similarity thresholds and small variance using real-world data sets.</abstract>
	</article>
	<article>
		<articleId>88</articleId>
		<title>Update Rewriting and Integrity Constraint Maintenance in a Schema Evolution Support System: PRISM++</title>
		<trackName>Research</trackName>
		<abstract>Supporting legacy applications when the database schema evolves represents a long-standing challenge of practical and theoretical importance. Recent work has produced algorithms and systems that automate the process of data migration and query adaptation; however, the problems of evolving integrity constraints and supporting legacy updates under schema and integrity constraints evolution are significantly more difficult and have thus far remained unsolved. In this paper, we address this issue by introducing a formal evolution model for the database schema structure and its integrity constraints, and use it to derive update mapping techniques akin to the rewriting techniques used for queries. Thus, we (i) propose a new set of Integrity Constraints Modification Operators (ICMOs), (ii) characterize the impact on integrity constraints of structural schema changes, (iii) devise representations that enable the rewriting of updates, and (iv) develop a unified approach for query and update rewriting under constraints. We then describe the implementation of these techniques provided by our PRISM++ system. The effectiveness of PRISM++ and its enabling technology has been verified on a testbed containing evolution histories of several scientific databases and web information systems, including the Genetic DB Ensembl (410+ schema versions in 9 years), and Wikipedia (240+ schema versions in 6 years).</abstract>
	</article>
	<article>
		<articleId>94</articleId>
		<title>The Complexity of Causality and Responsibility for Query Answers and non-Answers</title>
		<trackName>Research</trackName>
		<abstract>An answer to a query has a well-defined lineage expression (alternatively called how-provenance) that explains how the answer was derived. Recent work has also shown how to compute the lineage of a non-answer to a query. However, the cause of an answer or non-answer is a more subtle notion and consists, in general, of only a fragment of the lineage. In this paper, we adapt Halpern, Pearl, and Chockler's recent definitions of causality and responsibility to define the causes and responsibilities of answers and non-answers to queries. Responsibility captures the notion of degree of causality and serves to rank potentially many causes by their relative contributions to the effect.  Then, we study the complexity of computing causes and responsibilities for conjunctive queries. It is known that computing causes is in general NP-complete. Our first main result shows that all causes to conjunctive queries can be computed by a relational query which may involve negation. Thus, causality can be computed in PTIME, and very efficiently so. Next, we study computing responsibility. Here, we prove that the complexity depends on the conjunctive query and demonstrate a dichotomy between PTIME and NP-complete cases.  For the PTIME cases we give a non-trivial algorithm, consisting of a reduction to the max-flow computation problem. Finally, we prove that, even when it is in PTIME, responsibility is complete for LOGSPACE, implying that, unlike causality, it cannot be computed by a relational query.</abstract>
	</article>
	<article>
		<articleId>223</articleId>
		<title>Active Complex Event Processing over Event Streams</title>
		<trackName>Research</trackName>
		<abstract>State-of-the-art Complex Event Processing technology (CEP), while effective for pattern query execution, is limited in its capability of reacting to opportunities and risks detected by pattern queries. Especially reactions that affect the query results in turn have not been addressed in the literature. We propose to tackle these unsolved problems by embed- ding active rule support within the CEP engine, henceforth called Active CEP (ACEP). Active rules in ACEP allow us to specify a pattern query's dynamic condition and real-time actions. The technical challenge is to handle interactions between queries and reactions to queries in the high-volume stream execution. We hence introduce a novel stream-oriented transactional model along with a family of stream transaction scheduling algorithms that ensure the correctness of concurrent stream execution. We demonstrate the power of ACEP technology by applying it to the development of a healthcare system being deployed in UMass Medical School hospital. Through extensive performance experiments using real data streams, we show that our unique Low-Water-Mark stream transaction scheduler, customized for streaming environments, successfully achieves near-real- time system responsiveness and gives orders-of-magnitude better throughput than our alternative schedulers.</abstract>
	</article>
	<article>
		<articleId>227</articleId>
		<title>Structural Trend Analysis For Online Social Networks</title>
		<trackName>Research</trackName>
		<abstract>The identiﬁcation of popular and important topics discussed in social networks is crucial for a better understanding of societal concerns. It is also useful for users to stay on top of trends without having to sift through vast amounts of shared information. Trend detection methods introduced so far have not used the network topology and has thus not been able to distinguish viral topics from topics that are diffused mostly through the news media. To address this gap, we propose two novel structural trend deﬁnitions we call coordinated and uncoordinated trends that use friendship information to identify topics that are discussed among clustered and distributed users respectively. Our analyses and experiments show that structural trends are signiﬁcantly different from traditional trends and provide new insights into the way people share information online. We also propose a sampling technique for structural trend detection and prove that the solution yields in a gain in efﬁciency and is within an acceptable error bound. Experiments performed on a Twitter data set of 41.7 million nodes and 417 million posts show that even with a sampling rate of 0.005, the average precision is 0.93 for coordinated trends and 1 for uncoordinated trends. </abstract>
	</article>
	<article>
		<articleId>230</articleId>
		<title>Efficiently Compiling Efficient Query Plans for Modern Hardware</title>
		<trackName>Research</trackName>
		<abstract>As main memory grows, query performance is more and more determined by the raw CPU costs of query processing itself. The classical iterator style query processing technique is very simple and flexible, but shows poor performance on modern CPUs due to lack of locality and frequent instruction mis-predictions. Several techniques like batch oriented processing or vectorized tuple processing have been proposed in the past to improve this situation, but even these techniques are frequently out-performed by hand-written execution plans.  In this work we present a novel compilation strategy that translates a query into compact and efficient machine code using the LLVM compiler framework. By aiming at good code and data locality and predictable branch layout the resulting code frequently rivals the performance of hand-written C++ code. We integrated these techniques into the HyPer main memory database system and show that this results in excellent query performance while requiring only modest compilation time.</abstract>
	</article>
	<article>
		<articleId>243</articleId>
		<title>Distance-Constraint Reachability Computation in Uncertain Graphs</title>
		<trackName>Research</trackName>
		<abstract>Driven by the emerging network applications, querying and mining uncertain graphs has become increasingly important.  In this paper, we investigate a fundamental problem concerning uncertain graphs, which we call the distance- constraint reachability (DCR) problem: Given two vertices s and t, what is the probability that the distance from s to t is less than or equal to a user-defined threshold d in the uncertain graph? Since this problem is #P-Complete, we focus on efficiently and accurately approximating DCR online. Our main results include two new estimators for the probabilistic reachability. One is a Horvitz-Thomson type estimator based on the unequal probabilistic sampling scheme, and the other is a novel recursive sampling estimator, which effectively combines a deterministic recursive computational procedure with a sampling process to boost the estimation accuracy.  Both estimators can produce much smaller variance than the direct sampling estimator, which considers each trial to be either 1 or 0. We also present methods to make these estimators more computationally efficient.  The comprehensive experiment evaluation on both real and synthetic datasets demonstrates the efficiency and accuracy of our new estimators.</abstract>
	</article>
	<article>
		<articleId>244</articleId>
		<title>Compression Aware Physical Database Design</title>
		<trackName>Research</trackName>
		<abstract>Modern RDBMSs support the ability to compress data using methods such as null suppression and dictionary encoding. Data compression offers the promise of significantly reducing storage requirements and improving I/O performance for decision support queries. However, compression can also slow down update and query performance due to the CPU costs of compression and decompression. In this paper, we study how data compression affects choice of appropriate physical database design, such as indexes, for a given workload. We observe that approaches that decouple the decision of whether or not to choose an index from whether or not to compress the index can result in poor solutions. Thus, we focus on the novel problem of integrating compression into physical database design in a scalable manner.  We have implemented our techniques by modifying Microsoft SQL Server and the Database Engine Tuning Advisor (DTA) physical design tool. Our techniques are general and are potentially applicable to DBMSs that support other compression methods. Our experimental results on real world as well as TPC-H benchmark workloads demonstrate the effectiveness of our techniques.</abstract>
	</article>
	<article>
		<articleId>250</articleId>
		<title>Efficient Probabilistic Reverse Nearest Neighbor Query Processing on Uncertain Data</title>
		<trackName>Research</trackName>
		<abstract>Given a query object q, a reverse nearest neighbor (RNN) query in a common certain database returns the objects having q as their nearest neighbor. A new challenge for databases is dealing with uncertain objects. In this paper we consider probabilistic reverse nearest neighbor (PRNN) queries, which return the uncertain objects having the query object as nearest neighbor with a sufficiently high probability. We propose an algorithm for efficiently answering PRNN queries using new pruning mechanisms taking distance dependencies into account. We compare our algorithm to state-ofthe-art approaches recently proposed. Our experimental evaluation shows that our approach is able to significantly outperform previous approaches. In addition, we show how our approach can easily be extended to PRkNN (where k &gt; 1) query processing for which there is currently no efficient solution.</abstract>
	</article>
	<article>
		<articleId>252</articleId>
		<title>iCBS: Incremental Cost-based Scheduling under Piecewise Linear SLAs</title>
		<trackName>Research</trackName>
		<abstract>In a cloud computing environment, it is beneficial for the cloud service provider to offer differentiated services among different customers, who often have different cost profiles.  Therefore, cost-aware scheduling of queries is important.  A practical cost-aware scheduling algorithm must be able to handle the highly demanding query volumes in the scheduling queues to make online scheduling decisions very quickly.  We develop such a highly efficient cost-aware query scheduling algorithm, called iCBS. iCBS takes the query costs derived from the service level agreements (SLAs) between the service provider and its customers into account to make cost-aware scheduling decisions.  iCBS is an incremental variation of an existing scheduling algorithm, CBS. Although CBS exhibits an exceptionally good cost performance, it has a prohibitive time complexity.  Our main contributions are (1) to observe how CBS behaves under piecewise linear SLAs, which are very common in cloud computing systems, and (2) to efficiently leverage these observations and to reduce the online time complexity from O(n) for the original version CBS to O(log n) for iCBS.</abstract>
	</article>
	<article>
		<articleId>255</articleId>
		<title>CoHadoop: Flexible Data Placement and Its Exploitation in Hadoop</title>
		<trackName>Research</trackName>
		<abstract>Hadoop has become an attractive platform for large-scale data analytics. In this paper, we identify a major performance bottleneck of Hadoop: its lack of  ability to colocate related data on the same set of nodes. To overcome this bottleneck, we introduce CoHadoop, a lightweight extension of Hadoop that allows applications to control where data are stored. In contrast to previous approaches, CoHadoop retains the flexibility of Hadoop in that it does not require users to convert their data to a certain format (e.g., a relational database or a specific file format). Instead, applications give hints to CoHadoop that some set of files are related and may be processed jointly; CoHadoop then tries to colocate these files for improved efficiency. Our approach is designed such that the strong fault tolerance properties of Hadoop are retained. Colocation can be used to improve the efficiency of many operations, including indexing, grouping, aggregation, columnar storage, joins, and sessionization. We conducted a detailed study of joins and sessionization in the context of log processing - a common use case for Hadoop -, and propose efficient map-only algorithms that exploit colocated data partitions. In our experiments, we observed that CoHadoop outperforms both plain Hadoop and previous work. Our approach not only performs better than repartition-based algorithms, but also outperforms map-only algorithms that do exploit data partitioning but not colocation.</abstract>
	</article>
	<article>
		<articleId>268</articleId>
		<title>On Social-Temporal Group Query with Acquaintance Constraint</title>
		<trackName>Research</trackName>
		<abstract>Three essential criteria are important for activity planning, including: (1) finding a group of attendees familiar with the initiator, (2) ensuring each attendee in the group to have tight social relations with most of the members in the group, and (3) selecting an activity period available for all attendees. Therefore, this paper proposes Social-Temporal Group Query to find the activity time and attendees with the minimum total social distance to the initiator. Moreover, this query incorporates an acquaintance constraint to avoid finding a group with mutually unfamiliar attendees. Efficient processing of the social-temporal group query is very challenging. We show that the problem is NP-hard via a proof and formulate the problem with Integer Programming. We then propose two efficient algorithms, SGSelect and STGSelect, which include effective pruning techniques and employ the idea of pivot time slots to substantially reduce the running time, for finding the optimal solutions. Experimental results indicate that the proposed algorithms are much more efficient and scalable. In the comparison of solution quality, we show that STGSelect outperforms the algorithm that represents manual coordination by the initiator.</abstract>
	</article>
	<article>
		<articleId>278</articleId>
		<title>Merging What's Cracked, Cracking What's Merged: Adaptive Indexing in Main-Memory Column-Stores</title>
		<trackName>Research</trackName>
		<abstract>Adaptive indexing is characterized by the partial creation and refinement of the index as side effects of query execution. Dynamic or shifting workloads may benefit from preliminary index structures focused on the columns and specific key ranges actually queried - without incurring the cost of full index construction. The costs and benefits of adaptive indexing techniques should therefore be compared in terms of initialization costs, the overhead imposed upon queries, and the rate at which the index converges to a state that is fully-refined for a particular workload component.  Based on an examination of database cracking and adaptive merging, which are two techniques for adaptive indexing, we seek a hybrid technique that has a low initialization cost and also converges rapidly. We find the strengths and weaknesses of database cracking and adaptive merging complementary. One has a relatively high initialization cost but converges rapidly. The other has a low initialization cost but converges relatively slowly. We analyze the sources of their respective strengths and explore the space of hybrid techniques. We have designed and implemented a family of hybrid algorithms in the context of a full in-memory database system. Our experiments compare their behavior against database cracking and adaptive merging, as well as against both traditional full index lookup and scan of unordered data. We show that the new hybrids significantly improve over past methods while at least two of the hybrids come very close to the "ideal performance"' in terms of both overhead per query and convergence to a final state. </abstract>
	</article>
	<article>
		<articleId>283</articleId>
		<title>Keyword Search in Graphs: Finding r-cliques</title>
		<trackName>Research</trackName>
		<abstract>Keyword search over a graph finds a substructure of the graph containing all or some of the input keywords. Most of previous methods in this area find connected minimal trees that cover all the query keywords. Recently, it has been shown that finding subgraphs rather than trees can be more useful and informative for the users. However, the current tree or graph based methods may produce answers in which some content nodes (i.e., nodes that contain input keywords) are not very close to each other. In addition, when searching for answers, these methods may explore the whole graph rather than only the content nodes. This may lead to poor performance in execution time. To address the above problems, we propose the problem of finding r-cliques in graphs. An r-clique is a group of content nodes that cover all the input keywords and the distance between each two nodes is less than or equal to r. An exact algorithm is proposed that finds all r-cliques in the input graph. In addition, an approximation algorithm that produces r-cliques with 2-approximation in polynomial delay is proposed. Extensive performance studies using two large real data sets confirm the efficiency and accuracy of finding r-cliques in graphs. </abstract>
	</article>
	<article>
		<articleId>320</articleId>
		<title>Implementing Performance Competitive Logical Recovery</title>
		<trackName>Research</trackName>
		<abstract>New hardware platforms, e.g. cloud, multi-core, etc., have led to a reconsideration of database system architecture. Our Deuteronomy project separates transactional functionality from data management functionality, enabling a flexible response to exploiting new platforms. This separation requires, however, that recovery is described logically. In this paper, we extend current recovery methods to work in this logical setting. While this is straightforward in principle, performance is an issue. We show how ARIES style recovery optimizations can work for logical recovery where page information is not captured on the log. In side-by-side performance experiments using a common log, we compare logical recovery with a state-of-the art ARIES style recovery implementation and show that logical redo performance can be competitive.</abstract>
	</article>
	<article>
		<articleId>332</articleId>
		<title>An Incremental Hausdorff Distance Calculation Algorithm </title>
		<trackName>Research</trackName>
		<abstract>The Hausdorff distance is commonly used as a similarity measure between two point sets. Using this measure, a set X is considered similar to Y iff every point in X is close to at least one point in Y. Formally, the Hausdorff distance HausDist(X,Y) can be computed as the Max-Min distance from X to Y, i.e., find the maximum of the distance from an element in X to its nearest neighbor (NN) in Y. Although this is similar to the closest pair and farthest pair problems, computing the Hausdorff distance is a more challenging problem since its Max-Min nature involves both maximization and also minimization rather than just one or the other. A traditional approach to computing HausDist(X,Y) performs a linear scan over X and utilizes an index to help compute the NN in Y for each x in X. We present a pair of basic solutions that avoid scanning X by applying the concept of aggregate NN search to searching for the element in X that yields the Hausdorff distance. In addition, we propose a novel method which incrementally explores the indexes of the two sets X and Y simultaneously. As an example application of our techniques, we use the Hausdorff distance as a measure of similarity between two trajectories (represented as point sets). We also use this example application to compare the performance of our proposed method with the traditional approach and the basic solutions. Experimental results show that our proposed method outperforms all competitors by one order of magnitude in terms of the tree traversal cost and total response time.</abstract>
	</article>
	<article>
		<articleId>333</articleId>
		<title>Personalized Social Recommendations -  Accurate or Private?</title>
		<trackName>Research</trackName>
		<abstract>With the recent surge of social networks like Facebook, new forms of recommendations have become possible -  personalized recommendations of ads, content, and even new friend and product connections based on one's social interactions. Since recommendations may use sensitive social information, it is speculated that these recommendations are associated with privacy risks. The main contribution of this work is in formalizing these expected trade-offs between the accuracy and privacy of personalized social recommendations.  In this paper, we study whether "social recommendations'', or recommendations that are solely based on a user's social network, can be made without disclosing sensitive links in the social graph. More precisely, we quantify the loss in utility when existing recommendation algorithms are modified to satisfy a strong notion of privacy, called differential privacy. We prove lower bounds on the minimum loss in utility for any recommendation algorithm that is differentially private. We adapt two privacy preserving algorithms from the differential privacy literature to the problem of social recommendations, and analyze their performance in comparison to the lower bounds, both analytically and experimentally. We show that good private social recommendations are feasible only for a small subset of the users in the social network or for a lenient setting of privacy parameters.</abstract>
	</article>
	<article>
		<articleId>340</articleId>
		<title>Efficient Diversification of Web Search Results</title>
		<trackName>Research</trackName>
		<abstract>In this paper we analyze the efficiency of various search results diversification methods. While efficacy of diversification approaches has been deeply investigated in the past, response time and scalability issues have been rarely addressed.   A unified framework for studying performance and feasibility of result diversification solutions is thus proposed.  First we define a new methodology for detecting when, and how, query results need to be diversified. To this purpose, we rely on the concept of "query refinement" to estimate the probability of a query to be ambiguous. Then, relying on this novel ambiguity detection method, we deploy and compare on a standard test set, three different diversification methods: IASelect, xQuAD, and OptSelect. While the first two are recent state-of-the-art proposals, the latter is an original algorithm introduced in this paper. We evaluate both the efficiency and the effectiveness of our approach against its competitors by using the standard TREC Web diversification track testbed. Results shown that OptSelect is able to run two orders of magnitude faster than the two other state-of-the-art approaches and to obtain comparable figures in diversification effectiveness.</abstract>
	</article>
	<article>
		<articleId>344</articleId>
		<title>Social Content Matching in MapReduce</title>
		<trackName>Research</trackName>
		<abstract>Matching problems are ubiquitous. They occur in economic markets, labor markets, internet advertising, and elsewhere. In this paper we focus on an application of matching for social media. Our goal is to distribute content from information suppliers to information consumers. We seek to maximize the overall relevance of the matched content from suppliers to consumers while regulating the overall activity, e.g., ensuring that no consumer is overwhelmed with data and that all suppliers have chances to deliver their content.  We propose two matching algorithms, GreedyMR and StackMR, geared for the MapReduce paradigm. Both algorithms have provable approximation guarantees, and in practice they produce high-quality solutions. While both algorithms scale extremely well, we can show that StackMR requires only a poly-logarithmic number of MapReduce steps, making it an attractive option for applications with very large datasets. We experimentally show the trade-offs between quality and efficiency of our solutions on two large datasets coming from real-world social-media web sites. </abstract>
	</article>
	<article>
		<articleId>184</articleId>
		<title>PLP: Page Latch-free Shared-everything OLTP</title>
		<trackName>Research</trackName>
		<abstract>Scaling the performance of shared-everything on-line transaction processing to highly-parallel multicore hardware remains a great challenge for database system designers. Developments in OLTP technology remove locking and logging from being scalability bottlenecks on such systems, leaving page latching as the next potential problem. To tackle the page latching problem we design a system around physiological partitioning (PLP).   The PLP design applies logical-only partitioning, maintaining the desired properties of shared-everything designs, and introduces a multi-rooted B+Tree index structure (MRBTree) which allows us to partition the accesses at the physical page level. That is, logical partitioning along with MRBTrees ensure that all accesses to a given index page come from a single thread. Hence, those accesses (the most frequent type in OLTP workloads) can become entirely latch-free. We extend the design to make heap page accesses thread-private as well. The evaluation of a prototype PLP OLTP system shows that it eliminates page latching and achieves higher single-thread performance by simplifying key code paths in the system such as B+Tree operations. More importantly, the PLP system executes up to 7x fewer potentially contented critical sections per transaction, achieving better scalability and up to 38% higher performance on a multisocket multicore machine.</abstract>
	</article>
	<article>
		<articleId>188</articleId>
		<title>gStore: Answering SPARQL Queries via Subgraph Matching</title>
		<trackName>Research</trackName>
		<abstract>Due to the increasing use of RDF data,efficient processing of SPARQL queries over large RDF datasets has become an important issue. However, existing solutions suffer from two limitations: 1) they cannot answer SPARQL queries with wildcards in a scalable manner; and 2) they cannot handle frequent updates in RDF repositories efficiently. Thus, most of them have to reprocess the dataset from scratch. In this paper, we propose a graph-based approach to store and query RDF data. Rather than mapping RDF triples into a relational database as most existing methods do, we store RDF data as a large graph. A SPARQL query is then converted into a corresponding subgraph matching query. In order to speed up query processing, we develop a novel index, VS*-tree, together with some effective pruning rules and efficient searching algorithms. Our method can answer exact SPARQL queries and queries with wildcards in a uniform manner. Furthermore, we propose an effective maintenance algorithm to handle online updates over RDF repositories. Extensive experiments confirm the efficiency and effectiveness of our solution.</abstract>
	</article>
	<article>
		<articleId>191</articleId>
		<title>Albatross: Lightweight Elasticity in Shared Storage Databases for the Cloud using Live Data Migration</title>
		<trackName>Research</trackName>
		<abstract>Database systems serving cloud platforms must serve large numbers of applications (or tenants). In addition to managing tenants with small data footprints, different schemas, and variable load patterns, such multitenant data platforms must minimize their operating costs by efficient resource sharing. When deployed over a pay-per-use infrastructure, elastic scaling and load balancing, enabled by low cost live migration of tenant databases, is critical to tolerate load variations while minimizing operating cost. However, existing databases - relational databases and Key-Value stores alike - lack low cost live migration techniques, thus resulting in heavy performance impact during elastic scaling. We present Albatross, a technique for live migration in a multitenant database serving OLTP style workloads where the persistent database image is stored in a network attached storage. Albatross migrates the database cache and the state of active transactions to ensure minimal impact on transaction execution while allowing transactions active during migration to continue execution. It also guarantees serializability while ensuring correctness during failures. Our evaluation using two OLTP benchmarks shows that Albatross can migrate a live tenant database with no aborted transactions, negligible impact on transaction latency and throughput both during and after migration, and an unavailability window as low as 300 ms.</abstract>
	</article>
	<article>
		<articleId>208</articleId>
		<title>Column-Oriented Storage Techniques for MapReduce</title>
		<trackName>Research</trackName>
		<abstract>Users of MapReduce often run into performance problems when they scale up their workloads. Many of the problems they encounter can be overcome by applying techniques learned from over three decades of research on parallel DBMSs. However, translating these techniques to a MapReduce implementation such as Hadoop presents unique challenges that can lead to new design choices. This paper describes how column-oriented storage techniques can be incorporated in Hadoop in a way that preserves its popular programming APIs.  We show that simply using binary storage formats in Hadoop can provide a 3x performance boost over the naive use of text files. We then introduce a column-oriented storage format that is compatible with the replication and scheduling constraints of Hadoop and show that it can speed up MapReduce jobs on real workloads by an order of magnitude. We also show that dealing with complex column types such as arrays, maps, and nested records, which are common in MapReduce jobs, can incur significant CPU overhead. Finally, we introduce a novel skip list column format and lazy record construction strategy that avoids deserializing unwanted records to provide an additional 1.5x performance boost. Experiments on a real intranet crawl are used to show that our column-oriented storage techniques can improve the performance of the map phase in Hadoop by as much as two orders of magnitude.</abstract>
	</article>
	<article>
		<articleId>209</articleId>
		<title>Entity Matching: How Similar Is Similar</title>
		<trackName>Research</trackName>
		<abstract>Entity matching that finds records referring to the same entity is an important operation in data cleaning and integration. Existing studies usually use a given similarity function to quantify the similarity of records, and focus on devising index structures and algorithms for efficient entity matching. However it is a big challenge to define ``how similar is similar'' for real applications, since it is rather hard to automatically select appropriate similarity functions. In this paper we attempt to address this problem. As there are a large number of similarity functions, and even worse thresholds may have infinite values, it is rather expensive to find appropriate similarity functions and thresholds. Fortunately, we have an observation that different similarity functions and thresholds have redundancy, and we have an opportunity to prune inappropriate similarity functions. To this end, we propose effective optimization techniques to eliminate such redundancy, and devise efficient algorithms to find the best similarity functions. The experimental results on both real and synthetic datasets show that our method achieves high accuracy and outperforms the baseline algorithms.</abstract>
	</article>
	<article>
		<articleId>214</articleId>
		<title>Automatic Optimization for MapReduce Programs</title>
		<trackName>Research</trackName>
		<abstract>The MapReduce distributed programming framework has become popular, despite evidence that current implementations are inefficient, requiring far more hardware than a traditional relational databases to complete similar tasks. MapReduce jobs are amenable to many traditional database query optimizations (B+Trees for selections, column-store- style techniques for projections, etc), but existing systems do not apply them, substantially because free-form user code obscures the true data operation being performed. For example, a selection in SQL is easily detected, but a selection in a MapReduce program is embedded in Java code along with lots of other program logic. We could ask the programmer to provide explicit hints about the program’s data semantics, but one of MapReduce’s attractions is precisely that it does not ask the user for such information.  This paper covers Manimal, which automatically analyzes MapReduce programs and applies appropriate data- aware optimizations, thereby requiring no additional help at all from the programmer. We show that Manimal successfully detects optimization opportunities across a range of data operations, and that it yields speedups of up to 1,121% on previously-written MapReduce programs.</abstract>
	</article>
	<article>
		<articleId>96</articleId>
		<title>Query Expansion Based on Clustered Results</title>
		<trackName>Research</trackName>
		<abstract>Query expansion is a functionality of search engines that suggests a set of related queries for a user-issued keyword query. Typical corpus-driven keyword query expansion approaches return popular words in the results as expanded queries. Using these approaches, the expanded queries may correspond to a subset of possible query semantics, and thus miss relevant results. To handle ambiguous queries and exploratory queries, whose result relevance is difficult to judge, we propose a new framework for keyword query expansion: we start with clustering the results according to user specified granularity, and then generate expanded queries, such that one expanded query is generated for each cluster whose result set should ideally be the corresponding cluster. We formalize this problem and show its APX-hardness. Then we propose two efficient algorithms named iterative single-keyword refinement and partial elimination based convergence, respectively, which effectively generate a set of expanded queries from clustered results that provide a classification of the original query results. We believe our study of generating an optimal query based on the ground truth of the query results not only has applications in query expansion, but has significance for studying keyword search quality in general.</abstract>
	</article>
	<article>
		<articleId>103</articleId>
		<title>CoPhy: A Scalable, Portable, and Interactive Index Advisor for Large Workloads</title>
		<trackName>Research</trackName>
		<abstract>Index tuning, i.e., selecting the indexes appropriate for a workload, is a crucial problem in database system tuning. In this paper, we solve index tuning for large problem instances that are common in practice, e.g., thousands of queries in the workload, thousands of candidate indexes and several hard and soft constraints. Our work is the first to reveal that the index tuning problem has a well structured space of solutions, and this space can be explored efficiently with well known techniques from linear optimization. Experimental results demonstrate that our approach outperforms state-of-the-art commercial and research techniques by a significant margin (up to an order of magnitude).</abstract>
	</article>
	<article>
		<articleId>110</articleId>
		<title>Guided Data Repair</title>
		<trackName>Research</trackName>
		<abstract>In this paper we present GDR, a Guided Data Repair framework that incorporates user feedback in the cleaning process to enhance and accelerate existing automatic repair techniques while minimizing user involvement. GDR consults the user on the updates that are most likely to be beneficial in improving data quality. GDR also uses machine learning methods to identify and apply the correct updates directly to the database without the actual involvement of the user on these specific updates. To rank potential updates for consultation by the user, we first group these repairs and quantify the utility of each group using the decision-theory concept of value of information (VOI). We then apply active learning to order updates within a group based on their ability to improve the learned model. User feedback is used to repair the database and to adaptively refine the training set for the model. We empirically evaluate GDR on a real-world dataset and show significant improvement in data quality using our user guided repairing process. We also, assess the trade-off between the user efforts and the resulting data quality.</abstract>
	</article>
	<article>
		<articleId>111</articleId>
		<title>Using Paxos to Build a Scalable, Consistent, and Highly Available Datastore </title>
		<trackName>Research</trackName>
		<abstract>Spinnaker is an experimental datastore that is designed to run on a large cluster of commodity servers in a single datacenter. It features key-based range partitioning, 3-way replication, and a transactional get-put API with the option to choose either strong or timeline consistency on reads. This paper describes Spinnaker's Paxos-based replication protocol. The use of Paxos ensures that a data partition in Spinnaker will be available for reads and writes as long a majority of its replicas are alive. Unlike traditional master-slave replication, this is true regardless of the failure sequence that occurs. We show that Paxos replication can be competitive with alternatives that provide weaker consistency guarantees. Compared to an eventually consistent datastore, we show that Spinnaker can be as fast or even faster on reads and only 5% to 10% slower on writes.</abstract>
	</article>
	<article>
		<articleId>119</articleId>
		<title>Fast Set Intersection in Memory</title>
		<trackName>Research</trackName>
		<abstract>Set intersection is a fundamental operation in information retrieval and database systems. This paper introduces linear space data structures to represent sets such that their intersection can be computed in a worst-case efficient way.  In general, given k (preprocessed) sets, with totally n elements, we will show how to compute their intersection in expected time O(n/(\sqrt(w))+kr), where r is the intersection size and w is the number of bits in a machine-word. In addition,we introduce a very simple version of this algorithm that has weaker asymptotic guarantees but performs even better in practice; both algorithms outperform the state of the art techniques in terms of execution time for both synthetic and real data sets and workloads.</abstract>
	</article>
	<article>
		<articleId>122</articleId>
		<title>QSkycube: Efficient Skycube Computation Using Point-Based Space Partitioning</title>
		<trackName>Research</trackName>
		<abstract>Skyline queries have gained considerable attention for multi-criteria analysis of large-scale datasets. However, the skyline queries are known to return too many results for high-dimensional data. To address this problem, a skycube is introduced to efficiently provide users with multiple skylines with different strengths. For efficient skycube construction, state-of-the-art algorithms amortized redundant computation among subspace skylines, or cuboids, either (1) in a bottom-up fashion with the principle of sharing result or (2) in a top-down fashion with the principle of sharing structure. However, we observed further room for optimization in both principles. This paper thus aims to design a more efficient skycube algorithm that shares multiple cuboids using more effective structures. Specifically, we first develop each principle by leveraging multiple parents and a skytree, representing recursive point-based space partitioning. We then design an efficient algorithm exploiting these principles. Experimental results demonstrate that our proposed algorithm is significantly faster than state-of-the-art skycube algorithms in extensive datasets.</abstract>
	</article>
	<article>
		<articleId>123</articleId>
		<title>Hyper-Local, Directions-Based Ranking of Places</title>
		<trackName>Research</trackName>
		<abstract>Studies find that at least 20% of web queries have local intent; and the fraction of queries with local intent that originate from mobile properties may be twice as high. The emergence of standardized support for location providers in web browsers, as well as of providers of accurate locations, enables so-called hyper-local web querying where the location of a user is accurate at a much finer granularity than with IP-based positioning.  This paper addresses the problem of determining the importance of points of interest, or places, in local-search results. In doing so, the paper proposes techniques that exploit logged directions queries. A query that asks for directions from a location a to a location b is taken to suggest that a user is interested in traveling to b and thus is a vote that location b is interesting. Such user generated directions queries are particularly interesting because they are numerous and contain precise locations.  Specifically, the paper proposes a framework that takes a user location and a collection of near-by places as arguments, producing a ranking of the places. The framework enables a range of aspects of directions queries to be exploited for the ranking of places, including the frequency with which places have been referred to in directions queries. Next, the paper proposes an algorithm and accompanying data structures capable of ranking places in response to hyper-local web queries. Finally, an empirical study with very large directions query logs offers insight into the potential of directions queries for the ranking of places and suggests that the proposed algorithm is suitable for use in real web search engines.</abstract>
	</article>
	<article>
		<articleId>3</articleId>
		<title>Foundations of Uncertain-Data Integration</title>
		<trackName>Research</trackName>
		<abstract>There has been considerable past work studying data integration and uncertain data in isolation. We develop the foundations for local-as-view (LAV) data integration when the sources being integrated are uncertain. We motivate two distinct settings for uncertain-data integration. We then define containment of uncertain databases in these settings, which allows us to express uncertain sources as views over a virtual mediated uncertain database. Next, we define consistency of a set of uncertain sources and show intractability of consistency-checking. We identify an interesting special case for which consistency-checking is polynomial. Finally, the notion of certain answers from traditional LAV data integration does not generalize to the uncertain setting, so we define a corresponding notion of correct answers. </abstract>
	</article>
	<article>
		<articleId>6</articleId>
		<title>Generating Efficient Execution Plans for Vertically Partitioned XML Databases</title>
		<trackName>Research</trackName>
		<abstract>Experience with relational systems has shown that distribution is an effective way of improving the scalability of query evaluation. In this paper, we show how distributed query evaluation can be performed in a vertically partitioned XML database system. We propose a novel technique for constructing distributed execution plans that is independent of local query evaluation strategies. We then present a number of optimizations that allow us to further improve the performance of distributed query execution. Finally, we present a response time-based cost model that allows us to pick the best execution plan for a given query and database instance. Based on an implementation of our techniques within a native XML database system, we verify that our execution plans take advantage of the parallelism in a distributed system and that our cost model is effective at identifying the most advantageous plans.</abstract>
	</article>
	<article>
		<articleId>16</articleId>
		<title>Distributed Threshold Querying of General Functions by a Difference of Monotonic Representation</title>
		<trackName>Research</trackName>
		<abstract>The goal of a threshold query is to detect all objects whose score exceeds a given threshold. This type of query is used in many settings, such as data mining, event triggering, and top-k selection. Often, threshold queries are performed over distributed data. Given database relations that are distributed over many nodes, an object's score is computed by aggregating the value of each attribute, applying a given scoring function over the aggregation, and thresholding the function's value. However, joining all the distributed relations to a central database might incur prohibitive overheads in bandwidth, CPU, and storage accesses. Efficient algorithms required to reduce these costs exist only for monotonic aggregation threshold queries and certain specific scoring functions.  We present a novel approach for efficiently performing general distributed threshold queries. To the best of our knowledge, this is the first general solution to the problem of performing such queries with arbitrary scoring functions. We first present a solution for monotonic functions, and then introduce a technique to solve for other functions by representing them as a difference of monotonic functions. Experiments with real-world data demonstrate the method's effectiveness in achieving low communication and access costs.  </abstract>
	</article>
	<article>
		<articleId>22</articleId>
		<title>A Generic Framework for Handling Uncertain Data with Local Correlations</title>
		<trackName>Research</trackName>
		<abstract>Data uncertainty is ubiquitous in many real-world applications such as sensor/RFID data analysis. In this paper, we investigate uncertain data that exhibit local correlations, that is, each uncertain object is only locally correlated with a small subset of data, while being independent of others. We propose a generic framework for dealing with this kind of uncertain and locally correlated data, in which we investigate a classical spatial query, nearest neighbor query, on uncertain data with local correlations (namely LC-PNN). Most importantly, to enable fast LC-PNN query processing, we propose a novel filtering technique via offline pre-computations to reduce the query search space. We demonstrate through extensive experiments the efficiency and effectiveness of our approaches.</abstract>
	</article>
	<article>
		<articleId>28</articleId>
		<title>SnipSuggest: Context-Aware Autocompletion for SQL</title>
		<trackName>Research</trackName>
		<abstract>In this paper, we present SnipSuggest, a system that provides on-the-go, context-aware assistance in the SQL composition process. SnipSuggest aims to help the increasing population of non-expert database users, who need to perform complex analysis on their large-scale datasets, but have difficulty writing SQL. As a user types a query, it recommends possible additions to various clauses in the query using relevant snippets collected from a log of past queries. SnipSuggest’s current capabilities include suggesting tables, views, and table-valued functions in the FROM clause, columns in the SELECT clause, predicates in the WHERE clause, columns in the GROUP BY clause, aggregates, and some support for subqueries. SnipSuggest adjusts its recommendations according to the context: as the user writes more of the query, it is able to provide more accurate suggestions. We evaluate SnipSuggest over two query logs: one from an undergraduate database course and another from the SDSS database. We show that SnipSuggest is able to recommend useful snippets with up to 93.7% accuracy (average precision), at an interactive response time. We also show that SnipSuggest outperforms naive approaches, such as recommending the most popular snippets first.</abstract>
	</article>
	<article>
		<articleId>34</articleId>
		<title>On Triangulation-based Dense Neighborhood Graph Discovery</title>
		<trackName>Research</trackName>
		<abstract>This paper introduces a new definition of dense subgraph pattern, the $DN$-graph.  $DN$-graph considers both the size of the sub-structure and the minimum level of interactions between any pair of the vertices.  The mining of $DN$-graphs inherits the difficulty of finding clique, the fully-connected subgraphs. We thus opt for approximately locating the $DN$-graphs using the state-of-the-art graph triangulation methods. Our solution consists of a family of algorithms, each of which targets a different problem setting. These algorithms are iterative, and utilize repeated scans through the triangles in the graph to approximately locate the $DN$-graphs. Each scan on the graph triangles improves the results. Since the triangles are not physically materialized, the algorithms have small memory footprint.  With our solution, the users can adopt a ``pay as you go'' approach. They have the flexibility to terminate the mining process once they are satisfied with the quality of the results.  As a result,  our algorithms can cope with semi-streaming environment where the graph edges cannot fit into main memory. Results of extensive performance study confirmed our claims.</abstract>
	</article>
	<article>
		<articleId>648</articleId>
		<title>Evaluation Strategies for Top-k Queries over Memory-Resident Inverted Indexes</title>
		<trackName>Industrial, Applications, and Experience</trackName>
		<abstract>Top-k retrieval over main-memory inverted indexes is at the core of many modern applications: from large scale web search and advertising platforms, to text extenders and content management systems. In these systems, queries are evaluated using two major families of algorithms: document-at-a-time (DAAT) and term-at-a-time (TAAT). DAAT and TAAT algorithms have been studied extensively in the research literature, but mostly in disk-based settings. In this paper, we present an analysis and comparison of several DAAT and TAAT algorithms used in Yahoo!'s production platform for online advertising. The low-latency requirements of online advertising systems mandate memory-resident indexes. We compare the performance of several query evaluation algorithms using two real-world ad selection datasets and query workloads. We show how some adaptations of the original algorithms for main memory setting have yielded significant performance improvement, reducing running time and cost of serving by 60% in some cases. In these results both the original and the adapted algorithms have been evaluated over memory-resident indexes, so the improvements are algorithmic and not due to the fact that the experiments used main memory indexes.</abstract>
	</article>
	<article>
		<articleId>659</articleId>
		<title>Consistent Synchronization Schemes for Workload Replay</title>
		<trackName>Industrial, Applications, and Experience</trackName>
		<abstract>Oracle Database Replay has been recently introduced in Oracle 11g as a novel tool to test relational database systems [9]. It involves recording the workload running on the database server in a production system, and subsequently replaying it on the database server in a test system. A key feature of workload replay that enables realistic reproduction of a real workload is synchronization. It is a mechanism that enforces specific ordering on the replayed requests that comprise the workload. It affects the level of request concurrency and the consistency of the replay results when compared to the captured workload. In this paper, we define the class of consistent replay synchronization schemes and study, for the first time, the spectrum they cover and the tradeoffs they present. We place the only scheme proposed so far [9], the one implemented in Oracle 11g Release 1, within the aforementioned spectrum and show that it is coarse-grained and more restrictive than necessary, often enforcing dependencies between calls that are independent. By enforcing needless waits, it decreases the level of possible concurrency and degrades performance. To overcome these drawbacks, we identify the best scheme within the spectrum; it is finer-grained than its counterparts and strikes the right balance across different tradeoffs: it enforces a partial ordering on the replayed calls that minimizes the number of required waits and maximizes the level of concurrency, without compromising consistency of the replay results. We have implemented the new scheme in Oracle 11g Release 2. Our experiments indicate that it produces better quality replays than the pre-existing one for major classes of workload.</abstract>
	</article>
	<article>
		<articleId>671</articleId>
		<title>Inspector Gadget: A Framework for Custom Monitoring and Debugging of Distributed Dataflows</title>
		<trackName>Industrial, Applications, and Experience</trackName>
		<abstract>We consider how to monitor and debug  query processing dataflows, in distributed environments such as Pig/Hadoop. Our work is motivated by a series of informal user interviews, which revealed that monitoring and debugging needs are both pressing and diverse. In response to these interviews, we created a framework for custom dataflow instrumentation, called Inspector Gadget (IG).  IG makes it easy to write a wide variety of monitoring and debugging behaviors, and attaches seamlessly to an existing, unmodified dataflow environment such as Pig. We have implemented a dozen user-requested tools in Inspector Gadget, each in just a few hundred lines of Java code. The performance overhead is modest in most cases.  Our Pig-based implementation of IG, called Penny, is slated for public release in mid-2011, in conjunction with the upcoming Apache Pig v0.9 release. </abstract>
	</article>
	<article>
		<articleId>682</articleId>
		<title>Online Expansion of Large-scale Data Warehouses</title>
		<trackName>Industrial, Applications, and Experience</trackName>
		<abstract>Modern data warehouses store exceedingly large amounts of data, often considered the crown jewels of an enterprise. The amount of data maintained in such data warehouses increases significantly over time---often at a  continuous pace, e.g., by gathering additional data or retaining data for longer periods to derive  additional business value, but occasionally also precipitously, e.g., when consolidating disparate data warehouses and Data Marts into a single database. Having to \emph{expand} a data warehouse with 100's of TB of data by a substantial portion, e.g., 100\% or more is a complex and disruptive maintenance operation as it typically involves some sort of dumping and reloading of data which requires substantial downtime.  In this paper we describe the methodology and mechanisms we developed in \GPDB{} to expand large-scale data warehouses in an \emph{online} fashion, i.e., without noticeable downtime. At the core of our approach is a set of robust and transactionally consistent primitives that enable efficient data movement. Special emphasis was put on usability and control that lets an administrator tailor the expansion  process to specific operational characteristics via priorities and schedules.  We present a number of experiments to quantify the impact of an on-going expansion on query workloads. </abstract>
	</article>
	<article>
		<articleId>708</articleId>
		<title>HIWAS: Enabling Technology for Analysis of Clinical Data in XML Documents</title>
		<trackName>Industrial, Applications, and Experience</trackName>
		<abstract>The information contained in large collections of clinical data can be used for many valuable purposes, such as epidemiological studies, evidence-based medicine, monitoring compliance with best clinical practices, and cost-benefit analyses.  However, the emerging standards for the electronic representation of clinical data, such as the Clinical Document Architecture (CDA) ‎[1], are very complex and new tools are required to effectively extract and utilize the information contained in these documents.   In this paper, we present HIWAS, a research prototype of a new tool that creates a structural summary of a collection of XML documents, thereby enabling users to find relevant information for a specific purpose within complex XML documents.  A HIWAS user can create a target model that contains just the information they need, in a simplified representation that can be queried efficiently and is compatible with existing relational business intelligence technology.  By making these complex XML documents digestible with conventional tools, HIWAS lowers a key barrier to meaningful use of aggregated clinical data. </abstract>
	</article>
	<article>
		<articleId>718</articleId>
		<title>Jaql: A Scripting Language for Large Scale Semistructured Data Analysis</title>
		<trackName>Industrial, Applications, and Experience</trackName>
		<abstract>This paper describes Jaql, a declarative scripting language for analyzing large semistructured datasets in parallel using Hadoop’s MapReduce framework. Jaql is currently used in IBM’s InfoSphere BigInsights [5] and Cognos Consumer Insight [9] products. Jaql’s design features are: (1) a flexible data model, (2) reusability, (3) varying levels of abstraction, and (4) scalability. Jaql’s data model is inspired by JSON and can be used to represent datasets that vary from flat, relational tables to collections of semistructured documents. A Jaql script can start without any schema and evolve over time from a partial to a rigid schema. Reusability is provided through the use of higher-order functions and by packaging related functions into modules. Most Jaql scripts work at a high level of abstraction for concise specification of logical operations (e.g., join), but Jaql’s notion of physical transparency also provides a lower level of abstraction if necessary. This allows users to pin down the evaluation plan of a script for greater control or even add new operators. The Jaql compiler automatically rewrites Jaql scripts so they can run in parallel on Hadoop. In addition to describing Jaql’s design, we present the results of scale-up experiments on Hadoop running Jaql scripts for intranet data analysis and log processing.</abstract>
	</article>
	<article>
		<articleId>727</articleId>
		<title>Auto-Grouping Emails For Faster E-Discovery</title>
		<trackName>Industrial, Applications, and Experience</trackName>
		<abstract>In this paper, we examine the application of various grouping techniques to help improve the efficiency and reduce the costs involved in an electronic discovery process. Specifically, we create coherent groups of email documents which characterize either a syntactic theme, a semantic theme or an email thread. All such grouped documents can be reviewed together leading to a faster and more consistent review of documents. Syntactic grouping of emails is based on near duplicate detection whereas semantic grouping is based on identifying concepts in the email content using information extraction. Email thread detection is achieved using a combination of segmentation and near duplicate detection. We present experimental results on the Enron corpus that suggest that these approaches can significantly reduce the review time and show that high precision and recall in identifying the groups can be achieved. We also describe how these techniques are integrated into the IBM eDiscovery Analyzer product offering.</abstract>
	</article>
	<article>
		<articleId>730</articleId>
		<title>Web Scale Taxonomy Cleansing</title>
		<trackName>Industrial, Applications, and Experience</trackName>
		<abstract>Large ontologies and taxonomies are automatically harvested from web-scale data. These taxonomies tend to be huge, noisy, and contains little context. As a result, cleansing and enriching those large-scale taxonomies becomes a great challenge. A natural way to enrich a taxonomy is to map the taxonomy to existing datasets that contain rich information. In this paper, we study the problem of matching two web scale taxonomies. Besides the scale of the problem, we address the challenge that the taxonomies may not contain enough context (such as attribute values). As existing entity resolution techniques are based directly or indirectly on attribute values as context, we must explore external evidence for entity resolution. Specifically, we explore positive and negative evidence in external data sources such as the web and in other taxonomies. To integrate positive and negative evidence, we formulate the entity resolution problem as a problem of finding optimal multi-way cuts in a graph. We analyze the complexity of the problem, and propose a Monte Carlo algorithm for finding greedy cuts. We conduct extensive experiments and compare our approach with three existing methods to demonstrate the advantage of our approach.</abstract>
	</article>
	<article>
		<articleId>737</articleId>
		<title>Bridging two worlds with RICE</title>
		<trackName>Industrial, Applications, and Experience</trackName>
		<abstract>The growing need to use large amounts of data as the basis for sophisticated business analysis conflicts with the current capabilities of statistical software systems as well as the functions provided by most modern databases.   We developed two novel approaches towards a solution for this basic conflict, based on the widely-used statistical software package R and the SAP In-Memory Computing Engine (IMCE).   We thereby propose an alternative data exchange mechanism with R. Instead of using standard SQL interfaces like JDBC or ODBC we introduced SQL-SHM, a shared memory-based data exchange to incorporate R's vertical data structure. Furthermore, we extended this approach to R-Op introducing R scripts equivalent to native database operations like join or aggregation within the execution plans.  With the calculation engine, IMCE provides a framework to model logical execution plans and thereby offers a convenient way to use the full functionality of R via SQL interface. Moreover, this enables us to run R scripts in parallel without the necessity of extending the R interpreter itself.</abstract>
	</article>
	<article>
		<articleId>742</articleId>
		<title>Tenzing - A SQL Implementation On The MapReduce  Framework</title>
		<trackName>Industrial, Applications, and Experience</trackName>
		<abstract>Tenzing is a query engine built on top of MapReduce for ad hoc analysis of Google data. Tenzing supports a mostly complete SQL implementation (with several extensions) combined with several key characteristics such as heterogeneity, high performance, scalability    , reliability, metadata awareness, low latency, support for columnar storage and structured data, and easy extensibility. Tenzing is currently used internally at Google by 1000+ employees and serves 10000+ queries per day over 1.5 petabytes of compressed data. In this paper, we     describe the architecture and implementation of Tenzing, and present benchmarks of typical analytical queries.</abstract>
	</article>
	<article>
		<articleId>747</articleId>
		<title>An Algebraic Approach for Data-Centric Scientific Workflows</title>
		<trackName>Industrial, Applications, and Experience</trackName>
		<abstract>Scientific workflows have emerged as a basic abstraction for structuring and executing scientific experiments in computational environments. In many situations, these workflows are computationally and data intensive, thus requiring execution in large-scale parallel computers. However, parallelization of scientific workflows remains low-level, ad-hoc and labor-intensive, which makes it hard to exploit optimization opportunities. To address this problem, we propose an algebraic approach (inspired by relational algebra) and a parallel execution model that enable automatic optimization of scientific workflows. We conducted a thorough validation of our approach using both a real oil exploitation application and synthetic data scenarios. The experiments were run in Chiron, a data-centric scientific workflow engine implemented to support our algebraic approach. Our experiments demonstrate performance improvements of up to 226% compared to an ad-hoc workflow implementation.</abstract>
	</article>
	<article>
		<articleId>762</articleId>
		<title>Citrusleaf: A Real-Time NoSQL DB which Preserves ACID</title>
		<trackName>Industrial, Applications, and Experience</trackName>
		<abstract>In this paper, we describe the Citrusleaf real-time distributed database platform that is built using the core principles of traditional database consistency and reliability while also being fast and flexible enough for use in high-performance applications like real-time bidding. In fact, Citrusleaf is unique among NoSQL databases for its ability to provide immediate consistency and ACID while still being able to consistently exceed the high performance and scalability standards required by demanding real-time applications. This paper describes how the Citrusleaf system achieves the marriage of traditional database reliability, including immediate consistency and ACID, with flexibility and operational efficiency.  Citrusleaf scales linearly at extremely high throughput while keeping response time in the sub-millisecond range as demonstrated by the test results presented here. This kind of performance has enabled Citrusleaf to become the underlying component of some of the world’s largest real-time bidding networks. </abstract>
	</article>
	<article>
		<articleId>663</articleId>
		<title>RAMP: A System for Capturing and Tracing Provenance in MapReduce Workflows</title>
		<trackName>Demonstrations</trackName>
		<abstract>RAMP (Reduce And Map Provenance) is an extension to Hadoop that supports provenance capture and tracing for workflows of MapReduce jobs. RAMP uses a wrapper-based approach, requiring little if any user intervention in most cases, while retaining Hadoop’s parallel execution and fault tolerance. We demonstrate RAMP on a real-world MapReduce workflow generated from a Pig script that performs sentiment analysis over Twitter data. We show how RAMP’s automatic provenance capture and tracing capabilities provide a convenient and efficient means of drilling-down and verifying output elements.</abstract>
	</article>
	<article>
		<articleId>673</articleId>
		<title>BROAD: Diversified Keyword Search in Databases</title>
		<trackName>Demonstrations</trackName>
		<abstract>Keyword search in databases has received a lot of attention in the database community as it is an effective approach for querying a database without knowing its underlying schema. However, keyword search queries often return too many results. One standard solution is to rank results such that the "best" results appear first. Still, this approach can suffer from redundancy problem where many high ranking results are in fact coming from the same part of the database and results in other parts of the database are missed completely.  In this demo, we propose the BROAD system which allows users to perform diverse, hierarchical browsing on keyword search results. Our system partitions the answer trees in the keyword search results by selecting k diverse representatives from the trees, separating the answer trees into k groups based on their similarity to the representatives and then recursively applying the partitioning for each group. By constructing summarized result for the answer trees in each of the k groups, we provide a way for users to quickly locate the results that they desire.</abstract>
	</article>
	<article>
		<articleId>676</articleId>
		<title>TrustedDB: A Trusted Hardware based Outsourced Database Engine</title>
		<trackName>Demonstrations</trackName>
		<abstract>TrustedDB is an outsourced database prototype that allows clients to execute SQL queries with privacy and under regulatory compliance constraints without having to trust the service provider. TrustedDB achieves this by leveraging server hosted tamper-proof trusted hardware in critical query processing stages.  TrustedDB does not limit the query expressiveness of supported queries. And, despite the cost overhead and performance limitations of trusted hardware, the costs per query are orders of magnitude lower than any (existing or) potential future software-only mechanisms. In this demo we will showcase TrustedDB in action and discuss its architecture.</abstract>
	</article>
	<article>
		<articleId>686</articleId>
		<title>IPL-P: In-Page Logging with PCRAM</title>
		<trackName>Demonstrations</trackName>
		<abstract>A great deal of research has been done on solid-state storage media such as flash memory and non-volatile memory in the past few years. While NAND-type flash memory is now considered a top alternative to magnetic disk drives, different types of non-volatile memory have also begun to appear in the market recently. Although some advocates of storage class memory (SCM) predicted that flash memory would give way to SCM in the very near future, we believe that they will co-exist, complementing each other, for a while until the hurdles in its manufacturing process are lifted and storage class memory becomes commercially competitive in both capacity and price. This demo presents an improved design of In-Page Logging (IPL) by augmenting it with Phase Change RAM (PCRAM) in its log area. IPL is a buffer and storage management strategy that has been proposed for flash memory database systems. Due to the byte-addressability of PCRAM and its faster speed for small reads and writes, the IPL scheme with PCRAM can improve the performance of flash memory database systems even further by storing frequent log records in PCRAM instead of flash memory. We report a few advantages of this new design that will make IPL more suitable for flash memory database systems.</abstract>
	</article>
	<article>
		<articleId>689</articleId>
		<title>HyPer-sonic Combined Transaction AND Query Processing</title>
		<trackName>Demonstrations</trackName>
		<abstract>In this demo we will prove that it is -- against common belief -- indeed possible to build a main-memory database system that achieves world-record transaction processing throughput and best-of-breed OLAP query response times in one system in parallel on the same database state. The two workloads of online transaction processing (OLTP) and online analytical processing (OLAP) present different challenges for database architectures. Currently, users with high rates of mission-critical transactions have split their data into two separate systems, one database for OLTP and one so-called data warehouse for OLAP. While allowing for decent transaction rates, this separation has many disadvantages including data freshness issues due to the delay caused by only periodically initiating the Extract Transform Load-data staging and excessive resource consumption due to maintaining two separate information systems. We present an efficient hybrid system, called HyPer, that can handle both OLTP and OLAP simultaneously by using hardware-assisted replication mechanisms to maintain consistent snapshots of the transactional data. HyPer is a main-memory database system that guarantees the full ACID properties for OLTP transactions and executes OLAP query sessions (multiple queries) on arbitrarily current and consistent snapshots. The utilization of the processor-inherent support for virtual memory management (address translation, caching, copy-on-write) yields both at the same time: unprecedentedly high transaction rates as high as 100,000+ transactions per second and very fast OLAP query response times on a single system executing both workloads in parallel. The performance analysis is based on a combined TPC-C and TPC-H benchmark.</abstract>
	</article>
	<article>
		<articleId>695</articleId>
		<title>GrouPeer: A System for Clustering PDMSs</title>
		<trackName>Demonstrations</trackName>
		<abstract>Sharing structured data in a PDMS is hard due to schema heterogeneity and peer autonomy. To overcome heterogeneity, peer databases employ mappings that partially match local information to that of their direct neighbors. Traditionally, a query is successively rewritten along the propagation path on each peer. This results in gradual query degradation and the inability to retrieve data pertinent to the original version, even from peers that store such data. This demonstration presents GrouPeer, a system that overcomes the query degradation problem and enables the dynamic clustering of the overlay according to the semantics of the peer data, utilizing normal query traffic. Peers are provided with a methodology that allows them to choose which rewritten version of a query to answer and discover remote information-rich sources. The demonstration illustrates the functionalities in the clustering mechanism of GrouPeer: approximate query rewriting, query similarity methodology, construction of new mappings, iterative learning process, employment of automatic schema matching, and proves the capability of the system to perform gradual semantic clustering and enable high quality answers to peer queries. </abstract>
	</article>
	<article>
		<articleId>696</articleId>
		<title>CerFix: A System for Cleaning Data with Certain Fixes</title>
		<trackName>Demonstrations</trackName>
		<abstract>We present CerFix, a data cleaning system that finds certain fixes for tuples at the point of data entry, i.e., fixes that are guaranteed correct. It is based on master data, editing rules and certain regions. Given some attributes of an input tuple that are validated (assured correct), editing rules tell us what other attributes to fix and how to correct them with master data. A certain region is a set of attributes that, if validated, warrant a certain fix for the entire tuple. We demonstrate the following facilities provided by CerFix: (1) a region finder to identify certain regions; (2) a data monitor to find certain fixes for input tuples, by guiding users to validate a minimal number of attributes; and (3) an auditing module to show what attributes are fixed and where the correct values come from.</abstract>
	</article>
	<article>
		<articleId>705</articleId>
		<title>Online Visualization of Geospatial Stream Data using the WorldWide Telescope</title>
		<trackName>Demonstrations</trackName>
		<abstract>This demo presents the ongoing effort to meld the stream query processing capabilities of Microsoft StreamInsight with the visualization capabilities of the WorldWide Telescope. This effort provides visualization opportunities to manage, analyze, and process real-time information that is of spatio-temporal nature. The demo scenario is based on detecting, tracking and predicting interesting patterns over historical logs and real-time feeds of earthquake data.</abstract>
	</article>
	<article>
		<articleId>706</articleId>
		<title>Debugging Data Exchange with Vagabond</title>
		<trackName>Demonstrations</trackName>
		<abstract>In this paper, we present Vagabond, a  system that uses a novel holistic approach to help users to understand  and debug data exchange scenarios. Developing such a scenario  is a complex and labor-intensive process where errors are often only  revealed in the target instance produced as the result of this  process.  This makes it very hard to debug such scenarios, especially for  non-power users. Vagabond aides a user in debugging by  automatically generating possible explanations for target instance  errors identified by the user. </abstract>
	</article>
	<article>
		<articleId>713</articleId>
		<title>CrowdDB: Query Processing with the VLDB Crowd</title>
		<trackName>Demonstrations</trackName>
		<abstract>Databases often give incorrect answers when data are missing or semantic understanding of the data is required. Processing such queries requires human input for providing the missing information, for performing computationally difficult functions, and for matching, ranking, or aggregating results based on fuzzy criteria.   In this demo we present CrowdDB, a hybrid database system that automatically uses crowdsourcing to integrate human input for processing queries that a normal database system cannot answer.  CrowdDB uses SQL both as a language to ask complex queries and as a way to model data stored electronically and provided by human input. Furthermore, queries are automatically compiled and optimized.  Special operators provide user interfaces in order to integrate and cleanse human input. Currently CrowdDB supports two crowdsourcing platforms: Amazon Mechanical Turk and our own mobile phone platform. During the demo, the mobile platform will allow the VLDB crowd to participate as workers and help answer otherwise impossible queries.</abstract>
	</article>
	<article>
		<articleId>731</articleId>
		<title>Analytics for the Real-Time Web</title>
		<trackName>Demonstrations</trackName>
		<abstract>With the emergence of mobile devices constantly connected to the Internet, the nature of user-generated data has changed on most Web 2.0 sites. Today, people produce and share data more often and the lifespan of the data is shorter. Analyzing this data leads to new requirements for analytical systems: real-time processing and database-intensive workloads. Driven by these requirements, we have developed a new system for real-time analytics. Our system extends a key-value store, Cassandra, with push-based processing, transactional task execution, and synchronization. To demonstrate our system, we have built a service to reorganize news sites using real-time feedback from social media.</abstract>
	</article>
	<article>
		<articleId>739</articleId>
		<title>DivDB: A System for Diversifying Query Results</title>
		<trackName>Demonstrations</trackName>
		<abstract>With the availability of very large databases, an exploratory query can easily lead to a vast answer set, typically based on an answer’s relevance (i.e., top-k, tf-idf) to the user query. Navigating through such an answer set requires huge effort and users give up after perusing through the first few answers, thus some interesting answers hidden further down the answer set can easily be missed. An approach to address this problem is to present the user with the most diverse among the answers based on some diversity criterion. In this demonstration we present DivDB, a system we built to provide query result diversification both for advanced and novice users. For the experienced users, who may want to test the performance of existing and new algorithms, we provide an SQL-based extension to formulate queries with diversification. As for the novice users, who may be more interested in the result rather than how to tune the various algorithms’ parameters, the DivDB system allows the user to provide a “hint” to the optimizer on speed vs. quality of result. Moreover, novice users can use an interface to dynamically change the tradeoff value between relevance and diversity in the result, and thus visually inspect the result as he/she interacts with this parameter. This is a great feature to the end user because finding a good tradeoff value is a very hard task and it depends on several variables (i.e., query parameters, evaluation algorithms, and dataset properties). In this demonstration we show a study of the DivDB system with two image databases that contain many images of the same object under different settings (e.g., different camera angle). We show how the DivDB helps users to iteratively inspect diversification in the query result, without the need to know how to tune the many different parameters of the several existing algorithms in the DivDB system.</abstract>
	</article>
	<article>
		<articleId>743</articleId>
		<title>HOMES: A Higher-Order Mapping Evaluation System</title>
		<trackName>Demonstrations</trackName>
		<abstract>We describe a system that integrates querying and query transformation in a single higher-order query language. The system allows users to write queries that integrate and combine  query transformations. The power of higher-order functions also allows one to succinctly write complex relational queries. Our demonstration shows the utility of the system, explains the implementation architecture on top of a relational DBMS, and explains  optimizations that combine subquery caching techniques from relational databases with sharing detection schemes from functional programming.</abstract>
	</article>
	<article>
		<articleId>745</articleId>
		<title>Proactive Detection and Repair of Data Corruption: Towards a Hassle-free Declarative Approach with Amulet</title>
		<trackName>Demonstrations</trackName>
		<abstract>Occasional corruption of stored data is an unfortunate byproduct of the complexity of modern systems. Hardware errors, software bugs, and mistakes by human administrators can corrupt important sources of data. The dominant practice  to deal with data corruption today involves administrators writing ad hoc scripts that run data-integrity tests at the application, database, file-system, and storage levels. This manual approach, apart from being tedious and error-prone, provides no understanding of the potential system unavailability and data loss if a corruption were to occur. We have  developed the Amulet system that can  verify the correctness of stored data proactively and continuously. This demonstration focuses on  the uses of Amulet and its technical innovations: (i)  a declarative language for administrators  to specify their objectives regarding the detection and repair of data corruption; (ii) optimization and execution algorithms to meet  the administrator's objectives robustly and with least cost using pay-as-you-go cloud resources; and (iii) timely notification when corruption is detected, allowing proactive repair of corruption before it impacts users and applications. </abstract>
	</article>
	<article>
		<articleId>752</articleId>
		<title>Automatic Workload Driven Index Defragmentation</title>
		<trackName>Demonstrations</trackName>
		<abstract>Queries that scan a B-Tree index can suffer significant I/O performance degradation due to index fragmentation. The task of determining if an index should be defragmented is challenging for database administrators (DBAs) since today’s database engines offer no support for quantifying the impact of defragmenting an index on query I/O performance. Furthermore, DBMSs only support defragmentation at the granularity of an entire B-Tree, which can be very restrictive since defragmentation is an expensive operation and workloads typically access different ranges of an index non-uniformly. We have developed techniques to address the above two challenges, and implemented a prototype of automatic workload driven index defragmentation functionality in Microsoft SQL Server.  We demonstrate this functionality by showing (a) how the system tracks the potential benefit of defragmenting an index on I/O performance at low overhead, (b) the ability to defragment a range of a B-Tree index online, and (c) how the cost/benefit trade-off can be controlled in a policy driven manner to enable automatic workload driven index defragmentation requiring minimal DBA intervention.</abstract>
	</article>
	<article>
		<articleId>755</articleId>
		<title>Whodunit: An Auditing Tool for Detecting Data Breaches</title>
		<trackName>Demonstrations</trackName>
		<abstract>Commercial database systems provide support to maintain an audit trail that can be analyzed offline to identify potential threats to data security. We present a tool that performs data auditing that asks for an audit trail of all users and queries that referenced sensitive data, for example “find all queries and corresponding users that referenced John Doe’s salary in the last six months”.  Our tool: (1) handles complex SQL queries including constructs such as grouping, aggregation and subqueries, (2) has privacy guarantees, and (3) incorporates novel optimization techniques for efficiently auditing a large workload of complex SQL queries.</abstract>
	</article>
	<article>
		<articleId>759</articleId>
		<title>EIRENE: Interactive Design and Refinement of Schema Mappings via Data Examples</title>
		<trackName>Demonstrations</trackName>
		<abstract>One of the first steps in the process of integrating information from multiple sources into a desired target format is to specify the relationships, called schema mappings, between the source schemas and the target schema. In this demonstration, we showcase a new methodology for designing these schema mappings. Our system Eirene interactively solicits data examples from the mapping designer in order to design a schema mapping between a source schema and a target schema. A data example, in this context, is a pair consisting of a source instance and a target instance showing the desired outcome of performing data exchange using the schema mapping being designed. One of the central parts of the system is a module that, given a set of such data examples, either returns a “best” fitting schema mapping, or reports that no fitting schema mapping exists.</abstract>
	</article>
	<article>
		<articleId>761</articleId>
		<title>DataSynth: Generating Synthetic Data using Declarative Constraints</title>
		<trackName>Demonstrations</trackName>
		<abstract>A variety of scenarios such as database system and application testing, data masking, and benchmarking require synthetic database instances, often having complex data characteristics. We present DataSynth, a flexible tool for generating synthetic databases.  DataSynth uses a simple and powerful declarative abstraction based on cardinality constraints to specify data characteristics, and uses sophisticated algorithms to efficiently generate database instances satisfying the specified characteristics.  The demo will showcase various features of DataSynth using two real-world data generation scenarios.</abstract>
	</article>
	<article>
		<articleId>772</articleId>
		<title>InfoNetOLAPer: Integrating InfoNetWarehouse and InfoNetCube with InfoNetOLAP</title>
		<trackName>Demonstrations</trackName>
		<abstract>To support efficient graph OLAP operations on information networks, we propose two significant intermediate infrastructures: InfoNetWarehouse and InfoNetCube. InfoNetWarehouse is designed with novelty to be the warehouse model for information networks, which provides topic-oriented, integrated, and multi-dimensional organizational solutions for Information networks. InfoNetCube is our proposed datacube implemention that serves the OLAP of information networks. We further integrate the two infrastructures with InfoNetOLAP module into a prototype called InfoNetOLAPer, which has the following noteworthy features: (1) The basic InfoNetWarehouse schema is well implemented based on SQL Server 2000, (2) InfoNetCube improves the efficiency of InfoNetOLAP by the pre-computation of InfoNetLattice, and (3) InfoNetOLAPer supports efficient I-OLAP and T-OLAP operations.</abstract>
	</article>
	<article>
		<articleId>774</articleId>
		<title>From SPARQL to MapReduce: The Journey Using a Nested TripleGroup Algebra</title>
		<trackName>Demonstrations</trackName>
		<abstract>MapReduce-based data processing platforms offer a promising approach for cost-effective and Web-scale processing of Semantic Web data. However, one major challenge is that this computational paradigm leads to high I/O and communication costs when processing tasks with several join operations typical in SPARQL queries. The goal of this demonstration is to show how a system RAPID+, an extension of Apache Pig, enables more efficient SPARQL query processing on MapReduce using an alternative query algebra called the Nested TripleGroup Algebra (NTGA). The demonstration will offer opportunities for users to explore NTGA-Hadoop query plans for different SPARQL query structures as well as explore relationships between query plans based on relational algebra operators and those using NTGA operators.</abstract>
	</article>
	<article>
		<articleId>783</articleId>
		<title>FuDoCS: A Web Service Composition System Based on Fuzzy Dominance for Preference Query Answering</title>
		<trackName>Demonstrations</trackName>
		<abstract>DaaS Web service composition is a powerful means to answer users’ complex queries. User preferences are a key aspect that must be taken into account in the composition scheme. In this paper, we present an approach to automatically compose DaaS Web services while taking into account the user preferences. User preferences are modeled using fuzzy sets. We use an RDF query rewriting algorithm to determine the relevant services. The fuzzy constraints of the relevant services are matched to those of the query using a set of matching methods. We rank-order services using a fuzzification of Pareto dominance, then compute the top-k service compositions. We propose also a method to improve the diversity of returned compositions while maintaining as possible the compositions with the highest scores. </abstract>
	</article>
	<article>
		<articleId>786</articleId>
		<title>A Demonstration of HYRISE - A Main Memory Hybrid Storage Engine</title>
		<trackName>Demonstrations</trackName>
		<abstract>We propose to demonstrate HYRISE, a main memory hybrid database system, which automatically partitions tables into vertical partitions consisting of variable numbers of columns based on access patterns to each table. Using an accurate model of cache misses, HYRISE is able to predict the performance of different partitionings, and to automatically select the best partitions using an automated database partitioning algorithm. Our demonstration will show the results of the physical partitioning based on different query workloads, allowing demo attendees to visualize, fine-tune, and modify the partitioning using a GUI. It will then show how the various physical designs affect the query plans and the performance of the database as a whole. Attendees can thus experiment with various physical models, and can grasp the potential of hybrid partitionings, which achieve a 20% to 400% performance improvement over pure all-column or all-row designs on our realistic hybrid workload derived from customer applications.</abstract>
	</article>
	<article>
		<articleId>790</articleId>
		<title>++Spicy: an Open-Source Tool for Second-Generation Schema Mapping and Data Exchange</title>
		<trackName>Demonstrations</trackName>
		<abstract>Recent results in schema-mapping and data-exchange research may be considered the starting point for a new generation of systems, capable of dealing with a significantly larger class of applications. In this paper we demonstrate the first of these second-generation systems, called ++Spicy. We introduce a number of scenarios from a variety of data management tasks, such as data fusion, data cleaning, and ETL, and show how, based on the system, schema mappings and data exchange techniques can be very effectively applied to these contexts. We compare ++Spicy to the previous generations of tools, to show that this is much-needed advancement in the field.</abstract>
	</article>
	<article>
		<articleId>793</articleId>
		<title>UpStream: A Storage-centric Load Management System for Real-time Update Streams</title>
		<trackName>Demonstrations</trackName>
		<abstract>UpStream is a framework for load management over data streams with update semantics. It provides a novel storage manager architecture that can be plugged into data stream processing engines for serving streaming applications that require low-staleness results over real-time continuous queries. We propose to demonstrate the key aspects of the UpStream architecture as well as its performance using two different application scenarios: One that models a continuously updating financial market dashboard, and another one that is based on an intelligent transportation system for monitoring moving vehicles on a road traffic network. The demonstration will illustrate how UpStream can provide low-staleness query results for these applications under highly overloaded situations, by using a number of update scheduling and storage management techniques. This will be done through a number of interactive visual monitoring tools for the application interface as well as for monitoring the run-time operation of the UpStream system itself.</abstract>
	</article>
	<article>
		<articleId>800</articleId>
		<title>MapReduce Programming and Cost-based Optimization? Crossing this Chasm with Starfish</title>
		<trackName>Demonstrations</trackName>
		<abstract>MapReduce has emerged as a viable competitor to database systems in big data analytics. MapReduce programs are being written for a wide variety of application domains including business data processing, text analysis, natural language processing, Web graph and social network analysis, and computational science. However, MapReduce systems lack a feature that has been key to the historical success of database systems, namely, cost-based optimization. A major challenge here is that, to the MapReduce system, a program consists of black-box map and reduce functions written in some programming language like Java, C++, Python, or Ruby. Starfish is a self-tuning system for big data analytics that includes, to our knowledge, the first Cost-based Optimizer for simple to arbitrarily complex MapReduce programs.  Starfish also includes a Profiler to collect detailed statistical information from unmodified MapReduce programs, and a What-if Engine for fine-grained cost estimation. This demonstration will present the profiling, what-if analysis, and cost-based optimization of MapReduce programs in Starfish. We will show how (non-expert) users can employ the Starfish Visualizer to (a) get a deep understanding of a MapReduce program's behavior during execution, (b) ask hypothetical questions on how the program's behavior will change when parameter settings, cluster resources, or input data properties change, and (c) ultimately optimize the program. </abstract>
	</article>
	<article>
		<articleId>801</articleId>
		<title>AIDA: An Online Tool for Accurate Disambiguation of Named Entities in Text and Tables</title>
		<trackName>Demonstrations</trackName>
		<abstract>We present AIDA, a framework for online tool for entity detection and disambiguation.  Given a natural-language text or a Web table, we map mentions of ambiguous names onto canonical entities like people or places, registered in a knowledge base like DBpedia, Freebase, or YAGO.  AIDA is a robust framework centred around collective disambiguation exploiting the prominence of entities, similarity between the context of the mention and its candidates, and the coherence among candidate entities for all mentions.  We have developed a Web-based online interface for AIDA where different formats of inputs can be processed on the fly, returning proper entities and showing intermediate steps of the disambiguation process.</abstract>
	</article>
	<article>
		<articleId>805</articleId>
		<title>Microsoft Codename “Montego”  - Data Import, Transformation, and Publication for Information Workers</title>
		<trackName>Demonstrations</trackName>
		<abstract>One of the fundamental problems in database systems today is deriving useful information from the untold quantities of data fragments that exist in the web’s data stores. Data is abundant, useful information is rare. This problem space plays host to many successful and innovative solutions from industry, and the open-source community.  Each solution has its strengths and weaknesses based upon their balance between utility and usability. In this paper, we demonstrate the innovation and unique approach to data mashups that Microsoft Codename “Montego” brings to the space. The “Montego” tool allows non-technical users to create complex data queries in a graphical environment they are familiar with, while making the full expressiveness of a query language available to professional users.  “Montego” operates both as a standalone client, where a user can launch it from an application like Excel® to import and manipulate data into a spreadsheet, or in as a cloud service, where a user can take the product of data transformation and publish its results into a database or to the web as an OData feed. </abstract>
	</article>
	<article>
		<articleId>807</articleId>
		<title>SocialSpamGuard: A Data Mining-Based Spam Detection System for Social Media Networks</title>
		<trackName>Demonstrations</trackName>
		<abstract>We have entered the era of social media networks represented by Facebook, Twitter, YouTube and Flickr. Internet users now spend more time on social networks than search engines. Business entities or public figures set up social networking pages to enhance direct interactions with online users. Social media systems heavily depend on users for content contribution and sharing. Information is spread across social networks quickly and effectively. However, at the same time social media networks become susceptible to different types of unwanted and malicious spammer or hacker actions. There is a crucial need in the society and industry for security solution in social media. In this demo, we propose SocialSpamGuard, a scalable and online social media spam detection system based on data mining for social network security. We employ our GAD clustering algorithm for large scale clustering and integrate it with the designed active learning algorithm to deal with the scalability and real-time detection challenges.</abstract>
	</article>
	<article>
		<articleId>161</articleId>
		<title>ZINC: Efficient Indexing for Skyline Computation</title>
		<trackName>PVLDB Rollover</trackName>
		<abstract>We present a new indexing method named ZINC (for Z-order Indexing with Nested Code) that supports efficient skyline computation for data with both totally as well as partially ordered attribute domains. The key innovation in ZINC is based on combining the strengths of the ZBtree, which is the state-of-the-art index method for computing skylines involving totally ordered domains, with a novel, nested coding scheme that succinctly maps partial orders into total orders. An extensive performance evaluation demonstrates that ZINC significantly outperforms the state-of-the-art TSS indexing scheme for skyline queries.</abstract>
	</article>
	<article>
		<articleId>163</articleId>
		<title>SXPath - Extending XPath towards Spatial Querying on Web Documents</title>
		<trackName>PVLDB Rollover</trackName>
		<abstract>Querying data from presentation formats like HTML, for purposes such as information extraction, requires the consideration of tree structures as well as the consideration of spatial relationships between laid out elements. The underlying rationale is that frequently the rendering of tree structures is very involved and undergoing more frequent updates than the resulting layout structure. Therefore, in this paper, we present Spatial XPath (SXPath), an extension of XPath 1.0 that allows for inclusion of spatial navigation primitives into the language resulting in conceptually simpler queries on Web documents. The SXPath language is based on a combination of a spatial algebra with formal descriptions of XPath navigation, and maintains polynomial time combined complexity. Practical experiments demonstrate the usability of SXPath.</abstract>
	</article>
	<article>
		<articleId>178</articleId>
		<title>Personalized Privacy Protection in Social Networks</title>
		<trackName>PVLDB Rollover</trackName>
		<abstract>Due to the popularity of social networks, many proposals have been proposed to protect the privacy of the networks. All these works as- sume that the attacks use the same background knowledge. However, in practice, different users have different privacy protect re- quirements. Thus, assuming the attacks with the same background knowledge does not meet the personalized privacy requirements, meanwhile, it looses the chance to achieve better utility by taking advantage of differences of users’ privacy requirements. In this paper, we introduce a framework which provides privacy preserving services based on user's personal privacy requests. We define three levels of protection requirements based on the gradually increasing attacker's background knowledge and combine the label generalization protection and the structure protection techniques (i.e. adding noise edges or nodes)  together to satisfy different users' protection requirements. We verify the effectiveness of the proposed framework through extensive experiments.</abstract>
	</article>
	<article>
		<articleId>310</articleId>
		<title>Large-Scale Collective Entity Matching</title>
		<trackName>PVLDB Rollover</trackName>
		<abstract>There have been several recent advancements in Machine Learning community on the Entity Matching (EM) problem. However, their lack of scalability has prevented them from being applied in practical settings on large real-life datasets. Towards this end, we propose a principled framework to scale any generic EM algorithm. Our technique consists of running multiple instances of the EM algorithm on small neighborhoods of the data and passing messages across neighborhoods to construct a global solution. We prove formal properties of our framework and experimentally demonstrate the effectiveness of our approach in scaling EM algorithms. </abstract>
	</article>
	<article>
		<articleId>842</articleId>
		<title>Surrogate Parenthood: Protected and Informative Graphs</title>
		<trackName>PVLDB Rollover</trackName>
		<abstract>Many applications, including provenance and some analyses of social networks, require path-based queries over graph-structured data. When these graphs contain sensitive information, paths may be broken, resulting in uninformative query results. This paper presents innovative techniques that give users more informative graph query results; the techniques leverage a common industry practice of providing what we call surrogates: alternate, less sensitive versions of nodes and edges releasable to a broader community. We describe techniques for interposing surrogate nodes and edges to protect sensitive graph components, while maximizing graph connectivity and giving users as much information as possible. In this work, we formalize the problem of creating a protected account G' of a graph G. We provide a utility measure to compare the informativeness of alternate protected accounts and an opacity measure for protected accounts, which indicates the likelihood that an attacker can recreate the topology of the original graph from the protected account. We provide an algorithm to create a maximally useful protected account of a sensitive graph, and show through evaluation with the PLUS prototype that using surrogates and protected accounts adds value for the user, with no significant impact on the time required to generate results for graph queries.</abstract>
	</article>
	<article>
		<articleId>660</articleId>
		<title>Resiliency-Aware Data Management</title>
		<trackName>Special Challenges &amp; Visions</trackName>
		<abstract>Computing architectures change towards massively parallel environments with increasing numbers of heterogeneous components. The large scale in combination with decreasing feature sizes leads to dramatically increasing error rates. The heterogeneity further leads to new error types. Techniques for ensuring resiliency in terms of robustness regarding these errors are typically applied at hardware abstraction and operating system levels. However, as errors become the normal case, we observe increasing costs in terms of computation overhead for ensuring robustness. In this paper, we argue that ensuring resiliency on the data management level can reduce the required overhead by exploiting context knowledge of query processing and data storage. Apart from reacting on already detected errors, this was mostly neglected in database research so far. We therefore give a broad overview of the background of resilient computing and existing techniques from the database perspective. Based on the lack of existing techniques on data management level, we raise three fundamental challenges of resiliency-aware data management and present example use cases. Finally, our vision of resiliency-aware data management opens many directions of future work. Fundamental research, including the partial reuse of underlying mechanisms, would allow data management systems to cope with future hardware characteristics by effectively and efficiently ensuring resiliency. </abstract>
	</article>
	<article>
		<articleId>666</articleId>
		<title>Guided Interaction: Rethinking the Query-Result Paradigm</title>
		<trackName>Special Challenges &amp; Visions</trackName>
		<abstract>Many decades of research, coupled with continuous increases in computing power, have enabled highly efficient execution of queries on large databases. In consequence, for many databases, far more time is spent by users formulating queries than by the system evaluating them. It stands to reason that, looking at the overall query experience we provide users, we should pay attention to how we can assist users in the holistic process of obtaining the information they desire from the database, and not just the constituent activity of efficiently generating a result given a complete precise query. In this paper, we examine the conventional query-result paradigm employed by databases and demonstrate challenges encountered when following this paradigm for an example information seeking task. We recognize that the process of query specification itself is a major stumbling block. With current computational abilities, we are at a point where we can make use of the data in the database to aid in this pro- cess. To this end, we propose a new paradigm, guided interac- tion, to solve the noted challenges, by using interaction to guide the user through the query formulation, query execu- tion and result examination processes. The user can be given advance information during query specification that can not only assist in query formulation, but may also lead to aban- donment of an unproductive query direction or the satisfac- tion of information need even before the query specification is complete. There are significant engineering challenges to constructing the system we envision, and the building blocks to address these challenges exist today.</abstract>
	</article>
	<article>
		<articleId>694</articleId>
		<title>Data Generation for Application-Specific Benchmarking</title>
		<trackName>Special Challenges &amp; Visions</trackName>
		<abstract>The Transaction Processing Council (TPC) has played a pivotal role in the growth of the database industry over the last twenty-five years.  However, its handful of domain-specific benchmarks are increasingly irrelevant to the multitude of data-centric applications, and its top-down process is too slow.  This mismatch calls for a paradigm shift to a bottom-up community effort to develop tools for application-specific benchmarking.  Such a development program would center around techniques for synthetically scaling (up or down) an empirical dataset.  This engineering effort in turn requires the development of a database theory on attribute value correlation. </abstract>
	</article>
	<article>
		<articleId>726</articleId>
		<title>The Researcher's Guide to the Data Deluge: Querying a Scientific Database in Just a Few Seconds</title>
		<trackName>Special Challenges &amp; Visions</trackName>
		<abstract>There is a clear need for interactive exploration of extremely large databases, especially in the area of scientific data management where ingestion  of multiple Terabytes on a daily basis is foreseen. Unfortunately, current data management technology is not well-suited for such overwhelming demands.  In light of these challenges, we should rethink some of the strict requirements database systems adopted in the past. We envision that next generation database systems should interpret queries by their intent, rather than as a contract carved in stone for complete and correct answers. The result set should aid the user in understanding the database's content and provide guidance to continue the data exploration journey. A scientist can stepwise explore deeper and deeper into the database, and stop when the result quality drops below his satisfaction point. At the same time, response times should be close to instant such that they allow a scientist to interact with the system and explore the data in a contextualized way.  Several research directions are carved out to realize this vision. They range from engineering a novel database kernel where speed rather than completeness is the first class citizen, up to refusing to process a costly query in the first place, but providing advice on how to reformulate it instead, or even providing alternatives the system believes might be relevant for the exploration patterns observed. </abstract>
	</article>
	<article>
		<articleId>732</articleId>
		<title>Anthropocentric Data Systems</title>
		<trackName>Special Challenges &amp; Visions</trackName>
		<abstract>We present a vision for Anthropocentric Data Systems (ADS). First, ADS is about research that develops a conduit that  imports the input/wisdom of humans into the internals of data systems. Second, ADS is about research which discerns those tasks for which collective human input can actually improve system internals. Third, ADS is about researching the system architecture and structuring principles that will enable this fusion of automation and human feedback into the appropriate system internals. Finally, ADS is concerned with modeling human behavior and input, based on which such feedback can be anticipated, appropriately evaluated, and best exploited, as well as with appropriate interface systems, necessary to engage, visualize, and structure human input. We motivate and describe ADS and overview some key research challenges, including architectural principles, managing and organizing feedback (dealing with trust, maliciousness, and incentives), feedback aggregation and classification, modeling issues associating feedback to system components, prediction crowdsourcing, etc. In this, we attempt to highlight the relevant cross-disciplinary research. We also overview ongoing relevant research of our own that adds concreteness to the vision.  Given the unprecedented levels of human involvement into data systems recently and the various relevant R&amp;D results that are rapidly emerging from several different communities, the time is ripe to develop the next paradigm for creating, retrieving, and managing data, whereby  collective human wisdom plays an instrumental role. </abstract>
	</article>
	<article>
		<articleId>736</articleId>
		<title>Data Markets in the Cloud: An Opportunity for the Database Community</title>
		<trackName>Special Challenges &amp; Visions</trackName>
		<abstract>Cloud-computing is transforming many aspects of data management. Most recently, the cloud is seeing the emergence of digital markets for data and associated services.  We observe that our community has a lot to offer in building successful cloud-based data markets.  We outline some of the key challenges that such markets face and discuss the associated research problems that our community can help solve.</abstract>
	</article>
	<article>
		<articleId>808</articleId>
		<title>Data is Dead... Without What-if Models</title>
		<trackName>Special Challenges &amp; Visions</trackName>
		<abstract>Current database technology has raised the art of scalable DESCRIPTIVE analytics to a very high level. Unfortunately, what enterprises really need is PRESCRIPTIVE analytics to identify optimal business, policy, investment, and engineering decisions in the face of uncertainty. Such analytics, in turn, rest on deep PREDICTIVE analytics that go beyond mere statistical forecasting and are imbued with an understanding of the fundamental mechanisms that govern a system’s behavior, allowing what-if analyses. The database community needs to put what-if models and data on equal footing, developing systems that use both data and models to make sense of rich, real-world complexity and to support real-world decision-making. This model-and-data orientation requires significant extensions of many database technologies, such as data integration, query optimization and processing, and collaborative analytics. In this paper, we argue that data without what-if modeling may be the database community’s past, but data with what-if modeling must be its future.</abstract>
	</article>
	<article>
		<articleId>818</articleId>
		<title>Reverse Data Management</title>
		<trackName>Special Challenges &amp; Visions</trackName>
		<abstract>Database research mainly focuses on forward-moving data flows: source data is subjected to transformations and evolves through queries, aggregations, and view definitions to form a new target instance, possibly with a different schema. This Forward Paradigm underpins most data management tasks today, such as querying, data integration, data mining, etc. We contrast this forward processing with Reverse Data Management (RDM), where the action needs to be performed on the input data, on behalf of desired outcomes in the output data. Some data management tasks already fall under this paradigm, for example updates through views, data generation, data cleaning and repair. RDM is, by necessity, conceptually more difficult to define, and computationally harder to achieve.  Today, however, as increasingly more of the available data is derived from other data, there is an increased need to be able to modify the input in order to achieve a desired effect on the output, motivating a systematic study of RDM. We define the Reverse Data Management problem, and classify RDM problems into four categories.  We illustrate known examples of RDM problems and classify them under these categories, and introduce a new type of RDM problem, How-To Queries.</abstract>
	</article>
	<article>
		<articleId>822</articleId>
		<title>Exploring the Coming Repositories of Reproducible Experiments: Challenges and Opportunities</title>
		<trackName>Special Challenges &amp; Visions</trackName>
		<abstract>Computational reproducibility efforts in many communities will soon give rise to validated software and data repositories of high quality. A scientist in a field may want to query the components of such repositories to build new software workflows, perhaps after adding  the scientist's own algorithms.  This paper explores research challenges  necessary to achieving this goal.</abstract>
	</article>
	<article>
		<articleId>838</articleId>
		<title>Databases will Visualize Queries too</title>
		<trackName>Special Challenges &amp; Visions</trackName>
		<abstract>Visual Query Languages study ways to help users compose queries with visual metaphors. Information Visualization studies automatic visualization techniques to help users understand and analyze data. Query Management focuses on ways to help users manage and re-use existing queries. We observe that there is a related research question across those three topics which has not received much attention, namely that of Query Visualization: How to visually represent a query to help users quickly understand its intent? Here we argue that the involved challenges are still markedly different from those of the other three, that a solution can considerably improve the usability of DBMSs, and that the topic is thus worthy of attention. We envision, that in a few years, there will be free, modular, and lightweight tools available that allow users to visualize and interpret their queries.</abstract>
	</article>
</articles>